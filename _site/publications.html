<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="application-name" content="URCS BEAR Lab" />
  <meta name="theme-color" content="#b00" />
  
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap"
    rel="stylesheet"
  />
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&display=swap" rel="stylesheet">

  <link
    href="https://use.fontawesome.com/releases/v5.13.0/css/all.css"
    rel="stylesheet"
  />
  <link href="/styles.css" rel="stylesheet" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="shortcut icon" href="/logo.png" />
  <link
    rel="icon"
    type="image/png"
    href="/assets/logo.png"
    sizes="250x250"
  />

  <link
    rel="alternate"
    type="application/rss+xml"
    title="URCS BEAR Lab"
    href="http://localhost:4000/feed.xml"
  />

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Publications | URCS BEAR Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="BEAR Lab at University of Rochester in Rochester, NY." />
<meta property="og:description" content="BEAR Lab at University of Rochester in Rochester, NY." />
<link rel="canonical" href="http://localhost:4000/publications.html" />
<meta property="og:url" content="http://localhost:4000/publications.html" />
<meta property="og:site_name" content="URCS BEAR Lab" />
<script type="application/ld+json">
{"headline":"Publications","@type":"WebPage","url":"http://localhost:4000/publications.html","description":"BEAR Lab at University of Rochester in Rochester, NY.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="content">
      <header class="bg-white">
        <div class="w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l">
        <!-- <div class="headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l"> -->
        <!-- <div class="headermenu w-100 mw8 pa2 flex flex-column">           -->
          <a href="/" class="dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red">
          <!-- <a href="/" class="dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2"> -->
            <!-- <picture> -->
              <!-- <source  
                srcset="/assets/logo-sphere-01-01-01.svg"
                type="image/svg+xml"              
              /> -->
              <!-- <source
                srcset="/assets/logo-light.webp"
                type="image/webp"
              /> -->
              <!-- <source  
                srcset="/assets/logo-sphere-03-01.png"
                type="image/png"
              />
              <img 
                class="logo"
                alt="A coarsely tesselated sphere colored in shades of gray."
                class="pl2 w3"
              /> -->              
            <!-- </picture> -->
              BEAR Lab
          </a>
          <!--  --><nav class="mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns">
            <!-- <nav class="mt2 lh-copy w-100 w-25 pa3 mr2"></nav> -->
            <div class="tc mt0-ns mt2">
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/index">Home</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/team">Team</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib cmu-red " href="/publications">Publications</a>
              <!-- <a
                class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  "
                href="/teaching"
                >Teaching</a
              > -->
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/contact">Contact</a>
            </div>
          </nav>
        </div>

      </header>

      <main>
        <section>
  <!-- <div class="pv4 bg-near-white">
    <div class="w-100 mw8 ph4-l ph3 pv4-l pv3 center">
      <h1 class="f2 lh-title measure mt3">
        Publications
      </h1>
      <p class="f4 measure lh-copy">We publish at premier venues in Human-Computer Interaction, such as ACM CHI or ACM UIST.</p>
    </div>
  </div> -->

  <div class="w-100 mw8 ph4-l ph3 center mt4">
    <!-- <div id="facets" class="dn flex flex-row flex-wrap">
  <div class="facet mr3" id="venue_tags">
    <strong>Venue</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="authors">
    <strong>Author</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="tags">
    <strong>Tag</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="type">
    <strong>Type</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="awards">
    <strong>Award</strong>
    <ul class="list pl0"></ul>
  </div>

</div>

<label id="only-highlight" class="dn">
  <input type="checkbox" id="highlight">
  Show only highlights
</label>

<p id="clear-filters" class="dn b pointer">
  <i class="fas fa-times-circle cmu-red" aria-hidden="true"></i> Clear all filters. <span id="count_hidden">X</span> of
  <span id="count_total">X</span> publications are hidden by the filters.
</p> -->

  <!-- <p >
    We publish at premier venues in Human-Computer Interaction, such as ACM CHI or ACM UIST.
  </p> -->


<article>
  
  

  
    <h2 class="year f3 ur-blue" id="y2023">2023</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-conespeech_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-conespeech">
<i class="fas fa-award cmu-red" title="Best Paper Nominee Award"></i> 

                                    
                    
                    <a href="/publications/2023-conespeech.html" class="black hover-cmu-red link">ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Haohua Liu, 
                  Yingtian Shi, 
                  Jingying Wang, 
                  Ruici Guo, 
                  Zisu Li, 
                  Xuhai Xu, 
                  Chun Yu, 
                  Yuntao Wang, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                IEEE VR
                2023
                </a>
                </span>
              </div>

              

              

              
              <div class="cmu-red mv1 b">
                Best Paper Nominee Award
                
              </div>
              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-conespeech.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://ieeexplore.ieee.org/abstract/document/10049667" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1109/TVCG.2023.3247085" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_conespeech-exploring-directional-speech-interaction-for-multi-person-remote-communication-in-virtual-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_conespeech-exploring-directional-speech-interaction-for-multi-person-remote-communication-in-virtual-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-drg_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-drg">

                                    
                    
                    <a href="/publications/2023-drg.html" class="black hover-cmu-red link">DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Chen Liang, 
                  Chi Hsia, 
                  Chun Yu, 
                  Yukang Yan, 
                  Yuntao Wang, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM IMWUT
                2023
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-drg.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3569463" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3569463" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_drg-keyboard-enabling-subtle-gesture-typing-on-the-fingertip-with-dual-imu-rings" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_drg-keyboard-enabling-subtle-gesture-typing-on-the-fingertip-with-dual-imu-rings">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;content&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;A Survey on Remote Assistance and Training in Mixed Reality Environments | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Catarina Gonçalves Fidalgo\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Catarina Gonçalves Fidalgo\&quot;},\&quot;description\&quot;:\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Survey on Remote Assistance and Training in Mixed Reality Environments\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Catarina Gonçalves Fidalgo,\n          Yukang Yan,\n          Hyunsung Cho,\n          Mauricio Sousa,\n          David Lindlbauer,\n          Joaquim Jorge.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Catarina Gonçalves Fidalgo\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Catarina Gonçalves Fidalgo&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-training-survey.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br&gt;\n&lt;br&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;amp;ouid=111813668969331422555&amp;amp;rtpof=true&amp;amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-privacy_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-privacy">

                                    
                    
                    <a href="/publications/2023-privacy.html" class="black hover-cmu-red link">Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yutao Wang, 
                  Zirui Cheng, 
                  Xin Yi, 
                  Yan Kong, 
                  Xueyang Wang, 
                  Xuhai Xu, 
                  Yukang Yan, 
                  Chun Yu, 
                  Shwetak Patel, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2023
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-privacy.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581425" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3544548.3581425" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_modeling-the-trade-off-of-privacy-preservation-and-activity-recognition-on-low-resolution-images" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_modeling-the-trade-off-of-privacy-preservation-and-activity-recognition-on-low-resolution-images">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;content&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;A Survey on Remote Assistance and Training in Mixed Reality Environments | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Catarina Gonçalves Fidalgo\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Catarina Gonçalves Fidalgo\&quot;},\&quot;description\&quot;:\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Survey on Remote Assistance and Training in Mixed Reality Environments\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Catarina Gonçalves Fidalgo,\n          Yukang Yan,\n          Hyunsung Cho,\n          Mauricio Sousa,\n          David Lindlbauer,\n          Joaquim Jorge.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Catarina Gonçalves Fidalgo\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Catarina Gonçalves Fidalgo&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-training-survey.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br&gt;\n&lt;br&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;amp;ouid=111813668969331422555&amp;amp;rtpof=true&amp;amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-phoneselect_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-phoneselect">

                                    
                    
                    <a href="/publications/2023-phoneselect.html" class="black hover-cmu-red link">Selecting Real-World Objects via User-Perspective Phone Occlusion</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yue Qin, 
                  Chun Yu, 
                  Wentao Yao, 
                  Jiachen Yao, 
                  Chen Liang, 
                  Yueting Weng, 
                  Yukang Yan, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2023
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-phoneselect.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3544548.3580696" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3544548.3580696" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_selecting-real-world-objects-via-user-perspective-phone-occlusion" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_selecting-real-world-objects-via-user-perspective-phone-occlusion">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-handtoface_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-handtoface">
<i class="fas fa-award cmu-red" title="Best Paper Honorable Mention Award"></i> 

                                    
                    
                    <a href="/publications/2023-handtoface.html" class="black hover-cmu-red link">Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Zisu Li, 
                  Chen Liang, 
                  Yuntao Wang, 
                  Yue Qin, 
                  Chun Yu, 
                  Yukang Yan, 
                  Mingming Fan, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2023
                </a>
                </span>
              </div>

              

              

              
              <div class="cmu-red mv1 b">
                Best Paper Honorable Mention Award
                
              </div>
              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-handtoface.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3544548.3581008" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3544548.3581008" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_enabling-voice-accompanying-hand-to-face-gesture-recognition-with-cross-device-sensing" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_enabling-voice-accompanying-hand-to-face-gesture-recognition-with-cross-device-sensing">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.&quot;,&quot;content&quot;:&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zisu Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handtoface.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zisu Li\&quot;},\&quot;description\&quot;:\&quot;Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\\\% for recognizing 3 gestures and 91.5\\\\% for recognizing 8 gestures (excluding the “empty” gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zisu Li,\n          Chen Liang,\n          Yuntao Wang,\n          Yue Qin,\n          Chun Yu,\n          Yukang Yan,\n          Mingming Fan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zisu Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zisu Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handtoface.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3\\% for recognizing 3 gestures and 91.5\\% for recognizing 8 gestures (excluding the \&quot;empty\&quot; gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;content&quot;:&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.&quot;,&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yu Jiang*\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-handavatar.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yu Jiang*\&quot;},\&quot;description\&quot;:\&quot;We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yu Jiang*,\n          Zhipeng Li*,\n          Mufei He,\n          David Lindlbauer,\n          Yukang Yan.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yu Jiang*\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yu Jiang*&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-handavatar.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users' hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users' hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar's potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://doi.org/10.1145/3544548.3581027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=4OiMxRwrJHM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GAvys0HLqw0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=EJwMmeSN01Q\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-handavatar_thumb.png')">

              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/4OiMxRwrJHM?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>
              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-handavatar">

                                    
                    
                    <a href="/publications/2023-handavatar.html" class="black hover-cmu-red link">HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yu Jiang*, 
                  Zhipeng Li*, 
                  Mufei He, 
                  David Lindlbauer, 
                  Yukang Yan.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2023
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-handavatar.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://doi.org/10.1145/3544548.3581027" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3544548.3581027" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                
                <a href="https://www.youtube.com/watch?v=GAvys0HLqw0" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                

                <!--
                 -->

                
                <label for="slide_handavatar-embodying-non-humanoid-virtual-avatars-through-hands" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_handavatar-embodying-non-humanoid-virtual-avatars-through-hands">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings {Jiang23, 
 author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, 
 title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, 
 year = {2023}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3544548.3581027}, 
 doi = {10.1145/3544548.3581027}, 
 keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, 
 location = {Hamburg, Germany}, 
 series = {CHI '23} 
 }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;content&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;content&quot;:&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n&quot;,&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;content&quot;:&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;ouid=111813668969331422555&amp;rtpof=true&amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;&quot;,&quot;id&quot;:&quot;/publications/2023-training-survey&quot;,&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;url&quot;:&quot;/publications/2023-training-survey.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;A Survey on Remote Assistance and Training in Mixed Reality Environments | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Catarina Gonçalves Fidalgo\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Catarina Gonçalves Fidalgo\&quot;},\&quot;description\&quot;:\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Survey on Remote Assistance and Training in Mixed Reality Environments\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Catarina Gonçalves Fidalgo,\n          Yukang Yan,\n          Hyunsung Cho,\n          Mauricio Sousa,\n          David Lindlbauer,\n          Joaquim Jorge.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Catarina Gonçalves Fidalgo\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Catarina Gonçalves Fidalgo&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-training-survey.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br&gt;\n&lt;br&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;amp;ouid=111813668969331422555&amp;amp;rtpof=true&amp;amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.&quot;,&quot;content&quot;:&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n&quot;,&quot;id&quot;:&quot;/publications/2023-phoneselect&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-privacy&quot;,&quot;path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;url&quot;:&quot;/publications/2023-privacy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;url&quot;:&quot;/publications/2023-phoneselect.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-handtoface&quot;,&quot;path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;url&quot;:&quot;/publications/2023-handtoface.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handtoface.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3581008&quot;,&quot;title&quot;:&quot;Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing&quot;,&quot;authors&quot;:[&quot;Zisu Li&quot;,&quot;Chen Liang&quot;,&quot;Yuntao Wang&quot;,&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Mingming Fan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581008&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zisu2023, author = {Li, Zisu and Liang, Chen and Wang, Yuntao and Qin, Yue and Yu, Chun and Yan, Yukang and Fan, Mingming and Shi, Yuanchun}, title = {Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581008}, doi = {10.1145/3544548.3581008}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {313}, numpages = {17}, keywords = {sensor fusion, acoustic sensing, hand gestures}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-handtoface&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-phoneselect.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Selecting Real-World Objects via User-Perspective Phone Occlusion | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-phoneselect.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Selecting Real-World Objects via User-Perspective Phone Occlusion\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Wentao Yao,\n          Jiachen Yao,\n          Chen Liang,\n          Yueting Weng,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-phoneselect.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Perceiving the region of interest (ROI) and target object by smartphones from the user’s first-person perspective can enable diverse spatial interactions. In this paper, we propose a novel ROI input method and a target selecting method for smartphones by utilizing the user-perspective phone occlusion. This concept of turning the phone into real-world physical cursor benefits from the proprioception, gets rid of the constraint of camera preview, and allows users to rapidly and accurately select the target object. Meanwhile, our method can provide a resizable and rotatable rectangular ROI to disambiguate dense targets. We implemented the prototype system by positioning the user’s iris with the front camera and estimating the rectangular area blocked by the phone with the rear camera simultaneously, followed by a target prediction algorithm with the distance-weighted Jaccard index. We analyzed the behavioral models of using our method and evaluated our prototype system’s pointing accuracy and usability. Results showed that our method is well-accepted by the users for its convenience, accuracy, and efficiency.\n\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3544548.3580696&quot;,&quot;title&quot;:&quot;Selecting Real-World Objects via User-Perspective Phone Occlusion&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Wentao Yao&quot;,&quot;Jiachen Yao&quot;,&quot;Chen Liang&quot;,&quot;Yueting Weng&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3580696&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Computational Interaction&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Yue2023, author = {Qin, Yue and Yu, Chun and Yao, Wentao and Yao, Jiachen and Liang, Chen and Weng, Yueting and Yan, Yukang and Shi, Yuanchun}, title = {Selecting Real-World Objects via User-Perspective Phone Occlusion}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3580696}, doi = {10.1145/3544548.3580696}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {531}, numpages = {13}, keywords = {smartphone interaction, object selection}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-phoneselect&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-privacy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-privacy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-privacy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Zirui Cheng,\n          Xin Yi,\n          Yan Kong,\n          Xueyang Wang,\n          Xuhai Xu,\n          Yukang Yan,\n          Chun Yu,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2023.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-privacy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3544548.3581425&quot;,&quot;title&quot;:&quot;Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Zirui Cheng&quot;,&quot;Xin Yi&quot;,&quot;Yan Kong&quot;,&quot;Xueyang Wang&quot;,&quot;Xuhai Xu&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581425&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Privacy&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zirui2023, author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun}, title = {Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images}, year = {2023}, isbn = {9781450394215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544548.3581425}, doi = {10.1145/3544548.3581425}, abstract = {}, booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, articleno = {589}, numpages = {15}, keywords = {Privacy, privacy preserving, ADLs, low-resolution image., visual privacy, activities of daily living}, location = {Hamburg, Germany}, series = {CHI '23} }&quot;,&quot;slug&quot;:&quot;2023-privacy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-training-survey.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;A Survey on Remote Assistance and Training in Mixed Reality Environments | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Catarina Gonçalves Fidalgo\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-training-survey.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Catarina Gonçalves Fidalgo\&quot;},\&quot;description\&quot;:\&quot;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. Please download the raw data here: Link to raw data\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Survey on Remote Assistance and Training in Mixed Reality Environments\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Catarina Gonçalves Fidalgo,\n          Yukang Yan,\n          Hyunsung Cho,\n          Mauricio Sousa,\n          David Lindlbauer,\n          Joaquim Jorge.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Catarina Gonçalves Fidalgo\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Catarina Gonçalves Fidalgo&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-training-survey.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a novel taxonomy based on collaboration degree, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance.\n&lt;br&gt;\n&lt;br&gt;\n&lt;strong&gt;Please download the raw data here:&lt;/strong&gt;\n&lt;a href=\&quot;https://docs.google.com/spreadsheets/d/18HDMB0WBFpfUoYBE0nrL0VUSQQhzqj0w/edit?usp=sharing&amp;amp;ouid=111813668969331422555&amp;amp;rtpof=true&amp;amp;sd=true\&quot;&gt;Link to raw data&lt;/a&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/10049704&quot;,&quot;title&quot;:&quot;A Survey on Remote Assistance and Training in Mixed Reality Environments&quot;,&quot;authors&quot;:[&quot;Catarina Gonçalves Fidalgo&quot;,&quot;Yukang Yan&quot;,&quot;Hyunsung Cho&quot;,&quot;Mauricio Sousa&quot;,&quot;David Lindlbauer&quot;,&quot;Joaquim Jorge&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247081&quot;,&quot;venue_location&quot;:&quot;Shanghai&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Training and Collaboration&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Fidalgo2023, \n author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, \n title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, \n year = {2023}, \n publisher = {IEEE}, \n keywords = {Mixed Reality, Remote Assistance, Remote training}, \n url = {https://ieeexplore.ieee.org/document/10049704}, \n doi = {10.1109/TVCG.2023.3247081}, \n location = {Shanghai, China}, \n series = {IEEE VR '2023} \n }&quot;,&quot;slug&quot;:&quot;2023-training-survey&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2023-training-survey_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2023-training-survey">

                                    
                    
                    <a href="/publications/2023-training-survey.html" class="black hover-cmu-red link">A Survey on Remote Assistance and Training in Mixed Reality Environments</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Catarina Gonçalves Fidalgo, 
                  Yukang Yan, 
                  Hyunsung Cho, 
                  Mauricio Sousa, 
                  David Lindlbauer, 
                  Joaquim Jorge.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                IEEE VR
                2023
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2023-training-survey.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                

                
                <a href="https://doi.org/10.1109/TVCG.2023.3247081" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_a-survey-on-remote-assistance-and-training-in-mixed-reality-environments" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_a-survey-on-remote-assistance-and-training-in-mixed-reality-environments">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings {Fidalgo2023, 
 author = {Fidalgo, Catarina and Yan, Yukang and Cho, Hyusung and Sousa, Mauricio and Lindlbauer, David and Jorge, Joaquim}, 
 title = {A Survey on Remote Assistance and Training in Mixed Reality Environments}, 
 year = {2023}, 
 publisher = {IEEE}, 
 keywords = {Mixed Reality, Remote Assistance, Remote training}, 
 url = {https://ieeexplore.ieee.org/document/10049704}, 
 doi = {10.1109/TVCG.2023.3247081}, 
 location = {Shanghai, China}, 
 series = {IEEE VR '2023} 
 }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2022">2022</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2022-bodyoffset_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2022-bodyoffset">

                                    
                    
                    <a href="/publications/2022-bodyoffset.html" class="black hover-cmu-red link">Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Zhipeng Li, 
                  Yu Jiang, 
                  Yihao Zhu, 
                  Ruijia Chen, 
                  Ruolin Wang, 
                  Yuntao Wang, 
                  Yukang Yan, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM IMWUT
                2022
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2022-bodyoffset.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3534590" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3472749.3474750" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_modeling-the-noticeability-of-user-avatar-movement-inconsistency-for-sense-of-body-ownership-intervention" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_modeling-the-noticeability-of-user-avatar-movement-inconsistency-for-sense-of-body-ownership-intervention">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2022-clenchclick_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2022-clenchclick">

                                    
                    
                    <a href="/publications/2022-clenchclick.html" class="black hover-cmu-red link">ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Xiyuan Shen, 
                  Yukang Yan, 
                  Chun Yu, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM IMWUT
                2022
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2022-clenchclick.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3550327" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3472749.3474750" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_clenchclick-hands-free-target-selection-method-leveraging-teeth-clench-for-augmented-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_clenchclick-hands-free-target-selection-method-leveraging-teeth-clench-for-augmented-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2022-colortodepth_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2022-colortodepth">

                                    
                    
                    <a href="/publications/2022-colortodepth.html" class="black hover-cmu-red link">Color-to-Depth Mappings as Depth Cues in Virtual Reality</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Zhipeng Li, 
                  Yikai Cui, 
                  Tianze Zhou, 
                  Yu Jiang, 
                  Yuntao Wang, 
                  Yukang Yan, 
                  Michael Nebeling, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM UIST
                2022
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2022-colortodepth.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3534590" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3526113.3545646" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_color-to-depth-mappings-as-depth-cues-in-virtual-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_color-to-depth-mappings-as-depth-cues-in-virtual-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2022-DiminishedReality_thumb.png')">

              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>
              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2022-DiminishedReality">

                                    
                    
                    <a href="/publications/2022-DiminishedReality.html" class="black hover-cmu-red link">Towards Understanding Diminished Reality</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yi Fei Cheng, 
                  Hang Yin, 
                  Yukang Yan, 
                  Jan Gugenheimer, 
                  David Lindlbauer.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2022
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2022-DiminishedReality.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3472749.3474750" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_towards-understanding-diminished-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_towards-understanding-diminished-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings {Cheng22, 
 author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, 
 title = {Towards Understanding Diminished Reality}, 
 year = {2022}, 
 isbn = {9781450391573}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3491102.3517452}, 
 doi = {10.1145/3491102.3517452}, 
 articleno = {549}, 
 numpages = {16}, 
 keywords = {Empirical study, Diminished Reality, Mediated Reality}, 
 location = {New Orleans, LA, USA}, 
 series = {CHI '22} 
 }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;content&quot;:&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n&quot;,&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;content&quot;:&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n&quot;,&quot;id&quot;:&quot;/publications/2023-drg&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-handavatar&quot;,&quot;path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;url&quot;:&quot;/publications/2023-handavatar.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-handavatar.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://doi.org/10.1145/3544548.3581027&quot;,&quot;title&quot;:&quot;HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands&quot;,&quot;authors&quot;:[&quot;Yu Jiang*&quot;,&quot;Zhipeng Li*&quot;,&quot;Mufei He&quot;,&quot;David Lindlbauer&quot;,&quot;Yukang Yan&quot;],&quot;doi&quot;:&quot;10.1145/3544548.3581027&quot;,&quot;venue_location&quot;:&quot;Hamburg, Germany&quot;,&quot;venue_url&quot;:&quot;https://chi2023.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Animiation&quot;,&quot;Hand tracking&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-30sec&quot;:&quot;4OiMxRwrJHM&quot;,&quot;video-suppl&quot;:&quot;GAvys0HLqw0&quot;,&quot;video-talk-15min&quot;:&quot;EJwMmeSN01Q&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Jiang23, \n author = {Jiang, Yu and Li, Zhipeng and He, Mufei and Lindlbauer, David and Yan, Yukang}, \n title = {HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands}, \n year = {2023}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3544548.3581027}, \n doi = {10.1145/3544548.3581027}, \n keywords = {virtual avatar, embodiment, Mixed Reality, gestural interaction}, \n location = {Hamburg, Germany}, \n series = {CHI '23} \n }&quot;,&quot;slug&quot;:&quot;2023-handavatar&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;url&quot;:&quot;/publications/2023-drg.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-drg.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Chen Liang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-drg.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-drg.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Chen Liang\&quot;},\&quot;description\&quot;:\&quot;We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user’s thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb’s touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Chen Liang,\n          Chi Hsia,\n          Chun Yu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Chen Liang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Chen Liang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://www.ubicomp.org/ubicomp-iswc-2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-drg.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3\\% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3569463\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3569463&quot;,&quot;title&quot;:&quot;DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings&quot;,&quot;authors&quot;:[&quot;Chen Liang&quot;,&quot;Chi Hsia&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Cancun, Mexico&quot;,&quot;venue_url&quot;:&quot;https://www.ubicomp.org/ubicomp-iswc-2023/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Text Entry&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{Liang20232, author = {Liang, Chen and Hsia, Chi and Yu, Chun and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun}, title = {DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings}, year = {2023}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {4}, url = {https://doi.org/10.1145/3569463}, doi = {10.1145/3569463}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jan}, articleno = {170}, numpages = {30}, keywords = {text entry, gesture keyboard, smart ring, fingertip interaction} }&quot;,&quot;slug&quot;:&quot;2023-drg&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2023-conespeech.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Haohua Liu,\n          Yingtian Shi,\n          Jingying Wang,\n          Ruici Guo,\n          Zisu Li,\n          Xuhai Xu,\n          Chun Yu,\n          Yuntao Wang,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2023/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2023\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Nominee Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2023-conespeech.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/abstract/document/10049667\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;content&quot;:&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n&quot;,&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;content&quot;:&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n&quot;,&quot;id&quot;:&quot;/publications/2022-faceori&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2023-conespeech&quot;,&quot;path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;url&quot;:&quot;/publications/2023-conespeech.html&quot;,&quot;relative_path&quot;:&quot;_publications/2023-conespeech.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2023,&quot;month&quot;:11,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/abstract/document/10049667&quot;,&quot;title&quot;:&quot;ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Haohua Liu&quot;,&quot;Yingtian Shi&quot;,&quot;Jingying Wang&quot;,&quot;Ruici Guo&quot;,&quot;Zisu Li&quot;,&quot;Xuhai Xu&quot;,&quot;Chun Yu&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1109/TVCG.2023.3247085&quot;,&quot;venue_location&quot;:&quot;Shanghai, China&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2023/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice interface&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;awards&quot;:&quot;Best Paper Nominee Award&quot;,&quot;bibtex&quot;:&quot;@ARTICLE{10049667, author={Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality}, year={2023}, volume={29}, number={5}, pages={2647-2657}, doi={10.1109/TVCG.2023.3247085}}&quot;,&quot;slug&quot;:&quot;2023-conespeech&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;url&quot;:&quot;/publications/2022-faceori.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;content&quot;:&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n&quot;,&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-colortodepth&quot;,&quot;path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;url&quot;:&quot;/publications/2022-colortodepth.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xiyuan Shen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-clenchclick.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xiyuan Shen\&quot;},\&quot;description\&quot;:\&quot;We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user’s hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xiyuan Shen,\n          Yukang Yan,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xiyuan Shen\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xiyuan Shen&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-clenchclick.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3550327\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-colortodepth.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Color-to-Depth Mappings as Depth Cues in Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-colortodepth.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Color-to-Depth Mappings as Depth Cues in Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yikai Cui,\n          Tianze Zhou,\n          Yu Jiang,\n          Yuntao Wang,\n          Yukang Yan,\n          Michael Nebeling,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-colortodepth.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Color-to-Depth Mappings as Depth Cues in Virtual Reality&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yikai Cui&quot;,&quot;Tianze Zhou&quot;,&quot;Yu Jiang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Michael Nebeling&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3526113.3545646&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{zhipeng20222, author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun}, title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality}, year = {2022}, isbn = {9781450393201}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3526113.3545646}, doi = {10.1145/3526113.3545646}, abstract = {}, booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology}, articleno = {80}, numpages = {14}, keywords = {Virtual reality, color-to-depth mapping, depth perception}, location = {Bend, OR, USA}, series = {UIST '22} }&quot;,&quot;slug&quot;:&quot;2022-colortodepth&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-faceori.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yutao Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-faceori.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-faceori.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yutao Wang\&quot;},\&quot;description\&quot;:\&quot;Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yutao Wang,\n          Jiexin Ding,\n          Ishan Chatterjee,\n          Farshid Salemi Parizi,\n          Yuzhou Zhuang,\n          Yukang Yan,\n          Shwetak Patel,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yutao Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yutao Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-faceori.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517698&quot;,&quot;title&quot;:&quot;FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones&quot;,&quot;authors&quot;:[&quot;Yutao Wang&quot;,&quot;Jiexin Ding&quot;,&quot;Ishan Chatterjee&quot;,&quot;Farshid Salemi Parizi&quot;,&quot;Yuzhou Zhuang&quot;,&quot;Yukang Yan&quot;,&quot;Shwetak Patel&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3569463&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensingg&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }&quot;,&quot;slug&quot;:&quot;2022-faceori&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2022-faceori_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2022-faceori">

                                    
                    
                    <a href="/publications/2022-faceori.html" class="black hover-cmu-red link">FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yutao Wang, 
                  Jiexin Ding, 
                  Ishan Chatterjee, 
                  Farshid Salemi Parizi, 
                  Yuzhou Zhuang, 
                  Yukang Yan, 
                  Shwetak Patel, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2022
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2022-faceori.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517698" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3569463" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_faceori-tracking-head-position-and-orientation-using-ultrasonic-ranging-on-earphones" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_faceori-tracking-head-position-and-orientation-using-ultrasonic-ranging-on-earphones">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{jiexin2022, author = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun}, title = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517698}, doi = {10.1145/3491102.3517698}, abstract = {}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {290}, numpages = {12}, keywords = {head pose estimation., earphone, Acoustic ranging, head orientation}, location = {New Orleans, LA, USA}, series = {CHI '22} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2021">2021</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;content&quot;:&quot;An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n&quot;,&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-clenchclick&quot;,&quot;path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;url&quot;:&quot;/publications/2022-clenchclick.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-clenchclick.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3550327&quot;,&quot;title&quot;:&quot;ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality&quot;,&quot;authors&quot;:[&quot;Xiyuan Shen&quot;,&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Technique&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{10.1145/3550327, author = {Shen, Xiyuan and Yan, Yukang and Yu, Chun and Shi, Yuanchun}, title = {ClenchClick: Hands-Free Target Selection Method Leveraging Teeth-Clench for Augmented Reality}, year = {2022}, issue_date = {September 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, url = {https://doi.org/10.1145/3550327}, doi = {10.1145/3550327}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {sep}, articleno = {139}, numpages = {26}, keywords = {augmented reality, EMG sensing, target selection, hands-free interaction} }&quot;,&quot;slug&quot;:&quot;2022-clenchclick&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Zhipeng Li\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-bodyoffset.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Zhipeng Li\&quot;},\&quot;description\&quot;:\&quot;An avatar mirroring the user’s movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users’ sense of body ownership in VR. We applied angular offsets to the avatar’s shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user’s arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model’s extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Zhipeng Li,\n          Yu Jiang,\n          Yihao Zhu,\n          Ruijia Chen,\n          Ruolin Wang,\n          Yuntao Wang,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Zhipeng Li\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Zhipeng Li&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2022/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-bodyoffset.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3534590\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2021-semanticadapt_thumb.png')">

              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>
              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2021-semanticadapt">

                                    
                    
                    <a href="/publications/2021-semanticadapt.html" class="black hover-cmu-red link">SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yi Fei Cheng, 
                  Yukang Yan, 
                  Xin Yi, 
                  Yuanchun Shi, 
                  David Lindlbauer.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2021
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2021-semanticadapt.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/10.1145/3472749.3474750" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                

                

                
                <a href="https://www.youtube.com/watch?v=VIqI5TWiVpw" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                

                <!--
                 -->

                
                <label for="slide_semanticadapt-optimization-based-adaptation-of-mixed-reality-layouts-leveraging-virtual-physical-semantic-connections" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_semanticadapt-optimization-based-adaptation-of-mixed-reality-layouts-leveraging-virtual-physical-semantic-connections">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings { Cheng2021, 
 author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, 
 title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, 
 year = {2021}, 
 isbn = {9781450375146}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3472749.3474750}, 
 doi = {10.1145/3472749.3474750}, 
 keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, 
 location = {Virtual Event, USA}, 
 series = {UIST '2021} 
 }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;content&quot;:&quot;Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n&quot;,&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-bodyoffset&quot;,&quot;path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;url&quot;:&quot;/publications/2022-bodyoffset.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-bodyoffset.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3534590&quot;,&quot;title&quot;:&quot;Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention&quot;,&quot;authors&quot;:[&quot;Zhipeng Li&quot;,&quot;Yu Jiang&quot;,&quot;Yihao Zhu&quot;,&quot;Ruijia Chen&quot;,&quot;Ruolin Wang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2022/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Motion Redirection&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{zhipeng2022, author = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun}, title = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {2}, url = {https://doi.org/10.1145/3534590}, doi = {10.1145/3534590}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {jul}, articleno = {64}, numpages = {26}, keywords = {user behavior modelling, Virtual Reality, visual illusion} }&quot;,&quot;slug&quot;:&quot;2022-bodyoffset&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Towards Understanding Diminished Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Towards Understanding Diminished Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Towards Understanding Diminished Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2022-DiminishedReality.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;Diminished reality (DR) refers to the concept of removing content from a user’s visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Towards Understanding Diminished Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Hang Yin,\n          Yukang Yan,\n          Jan Gugenheimer,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2022.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2022\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2022-DiminishedReality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Diminished reality (DR) refers to the concept of removing content from a user's visual environment. While its implementation is becoming feasible, it is still unclear how users perceive and interact in DR-enabled environments and what applications it benefits. To address this challenge, we first conduct a formative study to compare user perceptions of DR and mediated reality effects (e. g., changing the color or size of target elements) in four example scenarios. Participants preferred removing objects through opacity reduction (i. e., the standard DR implementation) and appreciated mechanisms for maintaining a contextual understanding of diminished items (e. g., outlining). In a second study, we explore the user experience of performing tasks within DR-enabled environments. Participants selected which objects to diminish and the magnitude of the effects when performing two separate tasks (video viewing, assembly). Participants were comfortable with decreased contextual understanding, particularly for less mobile tasks. Based on the results, we define guidelines for creating general DR-enabled environments.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/GwV4AZjJnZY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GwV4AZjJnZY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=l9ycUrf50TE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2021-reflectrack_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2021-reflectrack">

                                    
                    
                    <a href="/publications/2021-reflectrack.html" class="black hover-cmu-red link">ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yuzhou Zhuang, 
                  Yuntao Wang, 
                  Yukang Yan, 
                  Xuhai Xu, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM UIST
                2021
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2021-reflectrack.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3472749.3474805" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3478098" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_reflectrack-enabling-3d-acoustic-position-tracking-using-commodity-dual-microphone-smartphones" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_reflectrack-enabling-3d-acoustic-position-tracking-using-commodity-dual-microphone-smartphones">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2021-facesight_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2021-facesight">

                                    
                    
                    <a href="/publications/2021-facesight.html" class="black hover-cmu-red link">FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yueting Weng, 
                  Chun Yu, 
                  Yingtian Shi, 
                  Yuhang Zhao, 
                  Yukang Yan, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2021
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2021-facesight.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3411764.3445484" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3411764.3445484" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_facesight-enabling-hand-to-face-gesture-interaction-on-ar-glasses-with-a-downward-facing-camera-vision" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_facesight-enabling-hand-to-face-gesture-interaction-on-ar-glasses-with-a-downward-facing-camera-vision">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&quot;,&quot;content&quot;:&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n&quot;,&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2022-DiminishedReality&quot;,&quot;path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;url&quot;:&quot;/publications/2022-DiminishedReality.html&quot;,&quot;relative_path&quot;:&quot;_publications/2022-DiminishedReality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2022,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517452&quot;,&quot;title&quot;:&quot;Towards Understanding Diminished Reality&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Hang Yin&quot;,&quot;Yukang Yan&quot;,&quot;Jan Gugenheimer&quot;,&quot;David Lindlbauer&quot;],&quot;doi&quot;:&quot;10.1145/3472749.3474750&quot;,&quot;venue_location&quot;:&quot;New Orleans, LA, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2022.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Diminished Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;video-thumb&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-30sec&quot;:&quot;GwV4AZjJnZY&quot;,&quot;video-talk-15min&quot;:&quot;l9ycUrf50TE&quot;,&quot;bibtex&quot;:&quot;@inproceedings {Cheng22, \n author = {Cheng, Yi Fei and Yin, Hang and Yan, Yukang and Gugenheimer, Jan and Lindlbauer, David}, \n title = {Towards Understanding Diminished Reality}, \n year = {2022}, \n isbn = {9781450391573}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3491102.3517452}, \n doi = {10.1145/3491102.3517452}, \n articleno = {549}, \n numpages = {16}, \n keywords = {Empirical study, Diminished Reality, Mediated Reality}, \n location = {New Orleans, LA, USA}, \n series = {CHI '22} \n }&quot;,&quot;slug&quot;:&quot;2022-DiminishedReality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yi Fei Cheng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-semanticadapt.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yi Fei Cheng\&quot;},\&quot;description\&quot;:\&quot;We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yi Fei Cheng,\n          Yukang Yan,\n          Xin Yi,\n          Yuanchun Shi,\n          David Lindlbauer.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yi Fei Cheng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yi Fei Cheng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-semanticadapt.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated by MR systems for them to be beneficial for end users. We contribute an approach that formulates this challenges as combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. In contrast to prior work, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous environments. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic association into account, our approach decreased the number of manual interface adaptations by 37%.\n\n&lt;h3&gt;More information&lt;/h3&gt;\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt; available at &lt;a href=\&quot;https://github.com/ycheng14799/SemanticAdapt\&quot;&gt; https://github.com/ycheng14799/SemanticAdapt&lt;/a&gt;\n&lt;/p&gt;\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/ZR5MFh2jKIU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3472749.3474750\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ZR5MFh2jKIU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=VIqI5TWiVpw\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=V7Qi6Mdsxak\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2021-proximic_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2021-proximic">

                                    
                    
                    <a href="/publications/2021-proximic.html" class="black hover-cmu-red link">ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yue Qin, 
                  Chun Yu, 
                  Zhaoheng Li, 
                  Mingyuan Zhong, 
                  Yukang Yan, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2021
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2021-proximic.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445687" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3411764.3445687" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_proximic-convenient-voice-activation-via-close-to-mic-speech-detected-by-a-single-microphone" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_proximic-convenient-voice-activation-via-close-to-mic-speech-detected-by-a-single-microphone">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;content&quot;:&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n&quot;,&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;content&quot;:&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n&quot;,&quot;id&quot;:&quot;/publications/2021-reflectrack&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-semanticadapt&quot;,&quot;path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;url&quot;:&quot;/publications/2021-semanticadapt.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-semanticadapt.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://doi.org/10.1145/3472749.3474750&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3472749.3474750&quot;,&quot;title&quot;:&quot;SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections&quot;,&quot;authors&quot;:[&quot;Yi Fei Cheng&quot;,&quot;Yukang Yan&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;,&quot;David Lindlbauer&quot;],&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Adaptive User Interfaces&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-30sec&quot;:&quot;ZR5MFh2jKIU&quot;,&quot;video-suppl&quot;:&quot;VIqI5TWiVpw&quot;,&quot;video-talk-15min&quot;:&quot;V7Qi6Mdsxak&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Cheng2021, \n author = {Cheng, Yi Fei and Yan, Yukang and Yi, Xin and Shi, Yuanchun, and Lindlbauer, David}, \n title = {SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections}, \n year = {2021}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3472749.3474750}, \n doi = {10.1145/3472749.3474750}, \n keywords = {Mixed Reality, Computational interaction, Adaptive user interfaces}, \n location = {Virtual Event, USA}, \n series = {UIST '2021} \n }&quot;,&quot;slug&quot;:&quot;2021-semanticadapt&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;url&quot;:&quot;/publications/2021-reflectrack.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-reflectrack.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuzhou Zhuang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-reflectrack.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuzhou Zhuang\&quot;},\&quot;description\&quot;:\&quot;3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yuzhou Zhuang,\n          Yuntao Wang,\n          Yukang Yan,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yuzhou Zhuang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yuzhou Zhuang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2021/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM UIST\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-reflectrack.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm * 60cm * 60cm space and 22.1 mm in the 30cm * 30cm * 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3478098&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3472749.3474805&quot;,&quot;title&quot;:&quot;ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones&quot;,&quot;authors&quot;:[&quot;Yuzhou Zhuang&quot;,&quot;Yuntao Wang&quot;,&quot;Yukang Yan&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3478098&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2021/&quot;,&quot;venue_tags&quot;:[&quot;ACM UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yuzhou2021, author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun}, title = {ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones}, year = {2021}, isbn = {9781450386357}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472749.3474805}, doi = {10.1145/3472749.3474805}, abstract = {}, booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology}, pages = {1050–1062}, numpages = {13}, keywords = {smartphones, sound reflection, Acoustic tracking, FMCW}, location = {Virtual Event, USA}, series = {UIST '21} }&quot;,&quot;slug&quot;:&quot;2021-reflectrack&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yue Qin\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-proximic.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-proximic.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yue Qin\&quot;},\&quot;description\&quot;:\&quot;Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yue Qin,\n          Chun Yu,\n          Zhaoheng Li,\n          Mingyuan Zhong,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yue Qin\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yue Qin&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-proximic.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2021-hulamove_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2021-hulamove">

                                    
                    
                    <a href="/publications/2021-hulamove.html" class="black hover-cmu-red link">HulaMove: Using Commodity IMU for Waist Interaction</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Xuhai Xu, 
                  Jiahao Li, 
                  Tianyi Yuan, 
                  Liang He, 
                  Xin Liu, 
                  Yukang Yan, 
                  Yuntao Wang, 
                  Yuanchun Shi, 
                  Jennifer Mankoff, 
                  Anind K Dey.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2021
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2021-hulamove.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3411764.3445182" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3411764.3445182" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_hulamove-using-commodity-imu-for-waist-interaction" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_hulamove-using-commodity-imu-for-waist-interaction">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2020">2020</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;content&quot;:&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n&quot;,&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-proximic&quot;,&quot;path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;url&quot;:&quot;/publications/2021-proximic.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-proximic.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445687&quot;,&quot;title&quot;:&quot;ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone&quot;,&quot;authors&quot;:[&quot;Yue Qin&quot;,&quot;Chun Yu&quot;,&quot;Zhaoheng Li&quot;,&quot;Mingyuan Zhong&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445687&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yue2021, author = {Qin, Yue and Yu, Chun and Li, Zhaoheng and Zhong, Mingyuan and Yan, Yukang and Shi, Yuanchun}, title = {ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445687}, doi = {10.1145/3411764.3445687}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {8}, numpages = {12}, keywords = {activity recognition, voice input, sensing technique}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-proximic&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HulaMove: Using Commodity IMU for Waist Interaction | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HulaMove: Using Commodity IMU for Waist Interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-hulamove.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HulaMove: Using Commodity IMU for Waist Interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Jiahao Li,\n          Tianyi Yuan,\n          Liang He,\n          Xin Liu,\n          Yukang Yan,\n          Yuntao Wang,\n          Yuanchun Shi,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-hulamove.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-headcross_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-headcross">

                                    
                    
                    <a href="/publications/2020-headcross.html" class="black hover-cmu-red link">FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Yingtian Shi, 
                  Chun Yu, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM IMWUT
                2020
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2020-headcross.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3380983" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3380983" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_frownonerror-interrupting-responses-from-smart-speakers-by-facial-expressions" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_frownonerror-interrupting-responses-from-smart-speakers-by-facial-expressions">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-earbuddy_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-earbuddy">

                                    
                    
                    <a href="/publications/2020-earbuddy.html" class="black hover-cmu-red link">EarBuddy: Enabling On-Face Interaction via Wireless Earbuds</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Xuhai Xu, 
                  Haitian Shi, 
                  Xin Yi, 
                  Wenjia Liu, 
                  Yukang Yan, 
                  Yuanchun Shi, 
                  Alex Mariakakis, 
                  Jennifer Mankoff, 
                  Anind K Dey.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2020
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2020-earbuddy.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376836" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3313831.3376836" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_earbuddy-enabling-on-face-interaction-via-wireless-earbuds" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_earbuddy-enabling-on-face-interaction-via-wireless-earbuds">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;content&quot;:&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.&quot;,&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;content&quot;:&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.&quot;,&quot;id&quot;:&quot;/publications/2021-facesight&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2021-hulamove&quot;,&quot;path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;url&quot;:&quot;/publications/2021-hulamove.html&quot;,&quot;relative_path&quot;:&quot;_publications/2021-hulamove.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445182&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445182&quot;,&quot;title&quot;:&quot;HulaMove: Using Commodity IMU for Waist Interaction&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Jiahao Li&quot;,&quot;Tianyi Yuan&quot;,&quot;Liang He&quot;,&quot;Xin Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuntao Wang&quot;,&quot;Yuanchun Shi&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445182&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2021, author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K}, title = {HulaMove: Using Commodity IMU for Waist Interaction}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445182}, doi = {10.1145/3411764.3445182}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {503}, numpages = {16}, keywords = {on-device sensing, Waist interaction}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-hulamove&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;url&quot;:&quot;/publications/2021-facesight.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2021-facesight.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yueting Weng\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021-facesight.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021-facesight.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yueting Weng\&quot;},\&quot;description\&quot;:\&quot;We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yueting Weng,\n          Chun Yu,\n          Yingtian Shi,\n          Yuhang Zhao,\n          Yukang Yan,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yueting Weng\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yueting Weng&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2021.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2021\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2021-facesight.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06\\%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2021,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3411764.3445484&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3411764.3445484&quot;,&quot;title&quot;:&quot;FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision&quot;,&quot;authors&quot;:[&quot;Yueting Weng&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Yuhang Zhao&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3411764.3445484&quot;,&quot;venue_url&quot;:&quot;https://chi2021.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yueting2021, author = {Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun}, title = {FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445484}, doi = {10.1145/3411764.3445484}, abstract = {}, booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, articleno = {10}, numpages = {14}, keywords = {AR Glasses, Hand-to-Face Gestures, Computer Vision}, location = {Yokohama, Japan}, series = {CHI '21} }&quot;,&quot;slug&quot;:&quot;2021-facesight&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-headcross.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-headcross.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users’ hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Yingtian Shi,\n          Chun Yu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-headcross.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3380983\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-frownonerror_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-frownonerror">
<i class="fas fa-award cmu-red" title="Best Paper Honorable Mention Award"></i> 

                                    
                    
                    <a href="/publications/2020-frownonerror.html" class="black hover-cmu-red link">FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Chun Yu, 
                  Wengrui Zheng, 
                  Ruining Tang, 
                  Xuhai Xu, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2020
                </a>
                </span>
              </div>

              

              

              
              <div class="cmu-red mv1 b">
                Best Paper Honorable Mention Award
                
              </div>
              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2020-frownonerror.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376810" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3313831.3376836" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_frownonerror-interrupting-responses-from-smart-speakers-by-facial-expressions" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_frownonerror-interrupting-responses-from-smart-speakers-by-facial-expressions">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2019">2019</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;content&quot;:&quot;In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.&quot;,&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-headcross&quot;,&quot;path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;url&quot;:&quot;/publications/2020-headcross.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-headcross.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3380983&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3380983&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Yingtian Shi&quot;,&quot;Chun Yu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3380983&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2020/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yingtian2020, author = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun}, title = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays}, year = {2020}, issue_date = {March 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {4}, number = {1}, url = {https://doi.org/10.1145/3380983}, doi = {10.1145/3380983}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {mar}, articleno = {35}, numpages = {22}, keywords = {hands-free selection, crossing selection, head-based interaction} }&quot;,&quot;slug&quot;:&quot;2020-headcross&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-frownonerror.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Wengrui Zheng,\n          Ruining Tang,\n          Xuhai Xu,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-frownonerror.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2019-privatetalk_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2019-privatetalk">

                                    
                    
                    <a href="/publications/2019-privatetalk.html" class="black hover-cmu-red link">PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Chun Yu, 
                  Yingtian Shi, 
                  Minxing Xie.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2019
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2019-privatetalk.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3332165.3347950" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3332165.3347950" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_privatetalk-activating-voice-input-with-hand-on-mouth-gesture-detected-by-bluetooth-earphones" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_privatetalk-activating-voice-input-with-hand-on-mouth-gesture-detected-by-bluetooth-earphones">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2018">2018</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;content&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;previous&quot;:null,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Ke Sun\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Ke Sun\&quot;},\&quot;description\&quot;:\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Ke Sun,\n          Yuntao Wang,\n          Chun Yu,\n          Yukang Yan,\n          Hongyi Wen,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Ke Sun\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Ke Sun&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-float.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2018-eyesfree_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2018-eyesfree">

                                    
                    
                    <a href="/publications/2018-eyesfree.html" class="black hover-cmu-red link">Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Chun Yu, 
                  Xiaojuan Ma, 
                  Xin Yi, 
                  Ke Sun, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2018
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2018-eyesfree.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3173574.3173616" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3173574.3173616" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_eyes-free-target-acquisition-in-interaction-space-around-the-body-for-virtual-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_eyes-free-target-acquisition-in-interaction-space-around-the-body-for-virtual-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;content&quot;:&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n&quot;,&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-frownonerror&quot;,&quot;path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;url&quot;:&quot;/publications/2020-frownonerror.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-frownonerror.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376810&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376810&quot;,&quot;title&quot;:&quot;FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Wengrui Zheng&quot;,&quot;Ruining Tang&quot;,&quot;Xuhai Xu&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2020, author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun}, title = {FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376810}, doi = {10.1145/3313831.3376810}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {voice user interface, conversation interruption, facial expression}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-frownonerror&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xuhai Xu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-earbuddy.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xuhai Xu\&quot;},\&quot;description\&quot;:\&quot;Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        EarBuddy: Enabling On-Face Interaction via Wireless Earbuds\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xuhai Xu,\n          Haitian Shi,\n          Xin Yi,\n          Wenjia Liu,\n          Yukang Yan,\n          Yuanchun Shi,\n          Alex Mariakakis,\n          Jennifer Mankoff,\n          Anind K Dey.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xuhai Xu\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xuhai Xu&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-earbuddy.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2018-virtualgrasp_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2018-virtualgrasp">

                                    
                    
                    <a href="/publications/2018-virtualgrasp.html" class="black hover-cmu-red link">VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Chun Yu, 
                  Xiaojuan Ma, 
                  Xin Yi, 
                  Ke Sun, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM CHI
                2018
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2018-virtualgrasp.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3173574.3173652" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3173574.3173652" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_virtualgrasp-leveraging-experience-of-interacting-with-physical-objects-to-facilitate-digital-object-retrieval" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_virtualgrasp-leveraging-experience-of-interacting-with-physical-objects-to-facilitate-digital-object-retrieval">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;content&quot;:&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.&quot;,&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;content&quot;:&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n&quot;,&quot;id&quot;:&quot;/publications/2019-privatetalk&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2020-earbuddy&quot;,&quot;path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;url&quot;:&quot;/publications/2020-earbuddy.html&quot;,&quot;relative_path&quot;:&quot;_publications/2020-earbuddy.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376836&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3313831.3376836&quot;,&quot;title&quot;:&quot;EarBuddy: Enabling On-Face Interaction via Wireless Earbuds&quot;,&quot;authors&quot;:[&quot;Xuhai Xu&quot;,&quot;Haitian Shi&quot;,&quot;Xin Yi&quot;,&quot;Wenjia Liu&quot;,&quot;Yukang Yan&quot;,&quot;Yuanchun Shi&quot;,&quot;Alex Mariakakis&quot;,&quot;Jennifer Mankoff&quot;,&quot;Anind K Dey&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376836&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sensing&quot;,&quot;Voice Interface&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{xuhai2020, author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.}, title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds}, year = {2020}, isbn = {9781450367080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3313831.3376836}, doi = {10.1145/3313831.3376836}, abstract = {}, booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, pages = {1–14}, numpages = {14}, keywords = {face and ear interaction, wireless earbuds, gesture recognition}, location = {Honolulu, HI, USA}, series = {CHI '20} }&quot;,&quot;slug&quot;:&quot;2020-earbuddy&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;url&quot;:&quot;/publications/2019-privatetalk.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2019-privatetalk.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-privatetalk.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Yingtian Shi,\n          Minxing Xie.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-privatetalk.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is performed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user's hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the difference of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3332165.3347950&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3332165.3347950&quot;,&quot;title&quot;:&quot;PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Yingtian Shi&quot;,&quot;Minxing Xie&quot;],&quot;doi&quot;:&quot;10.1145/3332165.3347950&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Voice Interface&quot;,&quot;Sensing&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang2019, author = {Yan, Yukang and Yu, Chun and Shi, Yingtian and Xie, Minxing}, title = {PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones}, year = {2019}, isbn = {9781450368162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332165.3347950}, doi = {10.1145/3332165.3347950}, abstract = {}, booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, pages = {1013–1020}, numpages = {8}, keywords = {voice input, hand gesture}, location = {New Orleans, LA, USA}, series = {UIST '19} }&quot;,&quot;slug&quot;:&quot;2019-privatetalk&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-virtualgrasp.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-virtualgrasp.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;content&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;previous&quot;:null,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Ke Sun\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Ke Sun\&quot;},\&quot;description\&quot;:\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Ke Sun,\n          Yuntao Wang,\n          Chun Yu,\n          Yukang Yan,\n          Hongyi Wen,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Ke Sun\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Ke Sun&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-float.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2018-headgesture_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2018-headgesture">

                                    
                    
                    <a href="/publications/2018-headgesture.html" class="black hover-cmu-red link">HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Yukang Yan, 
                  Chun Yu, 
                  Xin Yi, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                ACM IMWUT
                2018
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2018-headgesture.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3287076" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3287076" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_headgesture-hands-free-input-approach-leveraging-head-movements-for-hmd-devices" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_headgesture-hands-free-input-approach-leveraging-head-movements-for-hmd-devices">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3 ur-blue" id="y2017">2017</h2>
    

    
      

      <div class="publication flex mt3" data-pub="{&quot;excerpt&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;content&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;content&quot;:&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.&quot;,&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;next&quot;:{&quot;excerpt&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;content&quot;:&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.&quot;,&quot;id&quot;:&quot;/publications/2018-headgesture&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-virtualgrasp&quot;,&quot;path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;url&quot;:&quot;/publications/2018-virtualgrasp.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-virtualgrasp.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173652&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173652&quot;,&quot;title&quot;:&quot;VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173652&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20182, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Yi, Xin and Sun, Ke and Shi, Yuanchun}, title = {VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173652}, doi = {10.1145/3173574.3173652}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {gesture, mapping, object selection}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-virtualgrasp&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;url&quot;:&quot;/publications/2018-headgesture.html&quot;,&quot;previous&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-headgesture.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-headgesture.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xin Yi,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM IMWUT\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-headgesture.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3287076\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3287076&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3287076&quot;,&quot;title&quot;:&quot;HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xin Yi&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3287076&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2019/&quot;,&quot;venue_tags&quot;:[&quot;ACM IMWUT&quot;],&quot;type&quot;:[&quot;Conference&quot;,&quot;Journal&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;ACM IMWUT&quot;,&quot;bibtex&quot;:&quot;@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }&quot;,&quot;slug&quot;:&quot;2018-headgesture&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;previous&quot;:{&quot;excerpt&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;content&quot;:&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n&quot;,&quot;id&quot;:&quot;/publications/2017-float&quot;,&quot;next&quot;:{&quot;id&quot;:&quot;/publications/2018-eyesfree&quot;,&quot;path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;url&quot;:&quot;/publications/2018-eyesfree.html&quot;,&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;previous&quot;:null,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Ke Sun\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Ke Sun\&quot;},\&quot;description\&quot;:\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Ke Sun,\n          Yuntao Wang,\n          Chun Yu,\n          Yukang Yan,\n          Hongyi Wen,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Ke Sun\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Ke Sun&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-float.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;relative_path&quot;:&quot;_publications/2018-eyesfree.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yukang Yan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-eyesfree.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yukang Yan\&quot;},\&quot;description\&quot;:\&quot;Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Yukang Yan,\n          Chun Yu,\n          Xiaojuan Ma,\n          Xin Yi,\n          Ke Sun,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Yukang Yan\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Yukang Yan&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            ACM CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-eyesfree.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;coming-soon&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3173574.3173616&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3173574.3173616&quot;,&quot;title&quot;:&quot;Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality&quot;,&quot;authors&quot;:[&quot;Yukang Yan&quot;,&quot;Chun Yu&quot;,&quot;Xiaojuan Ma&quot;,&quot;Xin Yi&quot;,&quot;Ke Sun&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3173574.3173616&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ACM CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Behavior Modeling&quot;],&quot;venue&quot;:&quot;ACM CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{yukang20183, author = {Yan, Yukang and Yu, Chun and Ma, Xiaojuan and Huang, Shuai and Iqbal, Hasan and Shi, Yuanchun}, title = {Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality}, year = {2018}, isbn = {9781450356206}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3173574.3173616}, doi = {10.1145/3173574.3173616}, abstract = {}, booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems}, pages = {1–13}, numpages = {13}, keywords = {eyes-free, target acquisition, proprioception, virtual reality}, location = {Montreal QC, Canada}, series = {CHI '18} }&quot;,&quot;slug&quot;:&quot;2018-eyesfree&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-float.html&quot;,&quot;url&quot;:&quot;/publications/2017-float.html&quot;,&quot;previous&quot;:null,&quot;relative_path&quot;:&quot;_publications/2017-float.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/logo.png\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;URCS BEAR Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.6.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | URCS BEAR Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Ke Sun\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-float.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;URCS BEAR Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2023-10-05T13:49:50-04:00\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;datePublished\&quot;:\&quot;2023-10-05T13:49:50-04:00\&quot;,\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;},\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-float.html\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Ke Sun\&quot;},\&quot;description\&quot;:\&quot;Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the “fat finger” problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              BEAR Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Ke Sun,\n          Yuntao Wang,\n          Chun Yu,\n          Yukang Yan,\n          Hongyi Wen,\n          Yuanchun Shi.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Ke Sun\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Ke Sun&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-float.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the \&quot;fat finger\&quot; problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (98.9%) and walking (71.5%) contexts.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;BEAR Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://www.cs.rochester.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Department of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://rochester.edu/\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;University of Rochester&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2023-10-05\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update October 2023&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;hidden&quot;:false,&quot;external&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3025453.3026027&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/pdf/10.1145/3025453.3026027&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;Ke Sun&quot;,&quot;Yuntao Wang&quot;,&quot;Chun Yu&quot;,&quot;Yukang Yan&quot;,&quot;Hongyi Wen&quot;,&quot;Yuanchun Shi&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3026027&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Input Techniques&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }&quot;,&quot;slug&quot;:&quot;2017-float&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2023-10-05 13:49:50 -0400&quot;}">
            

              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2017-float_thumb.png')">

              

              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2017-float">

                                    
                    
                    <a href="/publications/2017-float.html" class="black hover-cmu-red link">Changing the Appearance of Real-World Objects By Modifying Their Surroundings</a>
                    
                  
              </h3>

              <div class="mb2">
                
                  Ke Sun, 
                  Yuntao Wang, 
                  Chun Yu, 
                  Yukang Yan, 
                  Hongyi Wen, 
                  Yuanchun Shi.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2017
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                                  
                  
                  <a href="/publications/2017-float.html" class="mr3 dib">
                    <span class="cta">Show Details</span>
                  </a>
                  
                

                
                <a href="https://dl.acm.org/doi/pdf/10.1145/3025453.3026027" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3025453.3026027" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                

                

                <!--
                 -->

                
                <label for="slide_changing-the-appearance-of-real-world-objects-by-modifying-their-surroundings" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_changing-the-appearance-of-real-world-objects-by-modifying-their-surroundings">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{ke2017, author = {Sun, Ke and Wang, Yuntao and Yu, Chun and Yan, Yukang and Wen, Hongyi and Shi, Yuanchun}, title = {Float: One-Handed and Touch-Free Target Selection on Smartwatches}, year = {2017}, isbn = {9781450346559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3025453.3026027}, doi = {10.1145/3025453.3026027}, abstract = {}, booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, pages = {692–704}, numpages = {13}, keywords = {finger gesture, one-handed interaction, target selection, smartwatch, tilt}, location = {Denver, Colorado, USA}, series = {CHI '17} }</pre>
                  </div>
                </div>
                

                <!--
                

                

                
                -->
              </div>
            </div>
      </div>
    
  
</article>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

<script>
  (function(e){if("object"==typeof exports&&"undefined"!=typeof module)module.exports=e();else if("function"==typeof define&&define.amd)define([],e);else{var t;t="undefined"==typeof window?"undefined"==typeof global?"undefined"==typeof self?this:self:global:window,t.itemsjs=e()}})(function(){var W,xn=Math.max;return function(){function s(l,e,n){function t(d,r){if(!e[d]){if(!l[d]){var i="function"==typeof require&&require;if(!r&&i)return i(d,!0);if(o)return o(d,!0);var c=new Error("Cannot find module '"+d+"'");throw c.code="MODULE_NOT_FOUND",c}var a=e[d]={exports:{}};l[d][0].call(a.exports,function(e){var o=l[d][1][e];return t(o||e)},a,a.exports,s,l,e,n)}return e[d].exports}for(var o="function"==typeof require&&require,r=0;r<n.length;r++)t(n[r]);return t}return s}()({1:[function(e,t){"use strict";t.exports=e("./src/index")},{"./src/index":5}],2:[function(e,t,n){(function(){var e=Math.log,o=Math.floor,r=function(e){var t=new r.Index;return t.pipeline.add(r.trimmer,r.stopWordFilter,r.stemmer),e&&e.call(t,t),t};r.version="1.0.0",r.utils={},r.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),r.utils.asString=function(e){return void 0===e||null===e?"":e.toString()},r.EventEmitter=function(){this.events={}},r.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop();if("function"!=typeof t)throw new TypeError("last argument must be a function");e.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},r.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);this.events[e].splice(n,1),this.events[e].length||delete this.events[e]}},r.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)})}},r.EventEmitter.prototype.hasHandler=function(e){return e in this.events},r.tokenizer=function(e){return arguments.length&&null!=e&&void 0!=e?Array.isArray(e)?e.map(function(e){return r.utils.asString(e).toLowerCase()}):e.toString().trim().toLowerCase().split(r.tokenizer.separator):[]},r.tokenizer.separator=/[\s\-]+/,r.tokenizer.load=function(e){var t=this.registeredFunctions[e];if(!t)throw new Error("Cannot load un-registered function: "+e);return t},r.tokenizer.label="default",r.tokenizer.registeredFunctions={default:r.tokenizer},r.tokenizer.registerFunction=function(e,t){t in this.registeredFunctions&&r.utils.warn("Overwriting existing tokenizer: "+t),e.label=t,this.registeredFunctions[t]=e},r.Pipeline=function(){this._stack=[]},r.Pipeline.registeredFunctions={},r.Pipeline.registerFunction=function(e,t){t in this.registeredFunctions&&r.utils.warn("Overwriting existing registered function: "+t),e.label=t,r.Pipeline.registeredFunctions[e.label]=e},r.Pipeline.warnIfFunctionNotRegistered=function(e){var t=e.label&&e.label in this.registeredFunctions;t||r.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},r.Pipeline.load=function(e){var t=new r.Pipeline;return e.forEach(function(e){var n=r.Pipeline.registeredFunctions[e];if(n)t.add(n);else throw new Error("Cannot load un-registered function: "+e)}),t},r.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){r.Pipeline.warnIfFunctionNotRegistered(e),this._stack.push(e)},this)},r.Pipeline.prototype.after=function(e,t){r.Pipeline.warnIfFunctionNotRegistered(t);var n=this._stack.indexOf(e);if(-1==n)throw new Error("Cannot find existingFn");++n,this._stack.splice(n,0,t)},r.Pipeline.prototype.before=function(e,t){r.Pipeline.warnIfFunctionNotRegistered(t);var n=this._stack.indexOf(e);if(-1==n)throw new Error("Cannot find existingFn");this._stack.splice(n,0,t)},r.Pipeline.prototype.remove=function(e){var t=this._stack.indexOf(e);-1==t||this._stack.splice(t,1)},r.Pipeline.prototype.run=function(e){for(var t,n=[],o=e.length,r=this._stack.length,s=0;s<o;s++){t=e[s];for(var i=0;i<r&&(t=this._stack[i](t,s,e),void 0!==t&&""!==t);i++);void 0!==t&&""!==t&&n.push(t)}return n},r.Pipeline.prototype.reset=function(){this._stack=[]},r.Pipeline.prototype.toJSON=function(){return this._stack.map(function(e){return r.Pipeline.warnIfFunctionNotRegistered(e),e.label})},r.Vector=function(){this._magnitude=null,this.list=void 0,this.length=0},r.Vector.Node=function(e,t,n){this.idx=e,this.val=t,this.next=n},r.Vector.prototype.insert=function(e,t){this._magnitude=void 0;var n=this.list;if(!n)return this.list=new r.Vector.Node(e,t,n),this.length++;if(e<n.idx)return this.list=new r.Vector.Node(e,t,n),this.length++;for(var o=n,i=n.next;void 0!=i;){if(e<i.idx)return o.next=new r.Vector.Node(e,t,i),this.length++;o=i,i=i.next}return o.next=new r.Vector.Node(e,t,i),this.length++},r.Vector.prototype.magnitude=function(){if(this._magnitude)return this._magnitude;for(var e,t=this.list,n=0;t;)e=t.val,n+=e*e,t=t.next;return this._magnitude=Math.sqrt(n)},r.Vector.prototype.dot=function(e){for(var t=this.list,n=e.list,o=0;t&&n;)t.idx<n.idx?t=t.next:t.idx>n.idx?n=n.next:(o+=t.val*n.val,t=t.next,n=n.next);return o},r.Vector.prototype.similarity=function(e){return this.dot(e)/(this.magnitude()*e.magnitude())},r.SortedSet=function(){this.length=0,this.elements=[]},r.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},r.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},r.SortedSet.prototype.toArray=function(){return this.elements.slice()},r.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},r.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},r.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,r=n-t,i=t+o(r/2),s=this.elements[i];1<r;){if(s===e)return i;s<e&&(t=i),s>e&&(n=i),r=n-t,i=t+o(r/2),s=this.elements[i]}return s===e?i:-1},r.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,r=n-t,i=t+o(r/2),s=this.elements[i];1<r;)s<e&&(t=i),s>e&&(n=i),r=n-t,i=t+o(r/2),s=this.elements[i];return s>e?i:s<e?i+1:void 0},r.SortedSet.prototype.intersect=function(e){for(var t=new r.SortedSet,n=0,o=0,i=this.length,s=e.length,l=this.elements,a=e.elements;!0&&!(n>i-1||o>s-1);){if(l[n]===a[o]){t.add(l[n]),n++,o++;continue}if(l[n]<a[o]){n++;continue}if(l[n]>a[o]){o++;continue}}return t},r.SortedSet.prototype.clone=function(){var e=new r.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},r.SortedSet.prototype.union=function(e){var t,n,o;this.length>=e.length?(t=this,n=e):(t=e,n=this),o=t.clone();for(var r=0,i=n.toArray();r<i.length;r++)o.add(i[r]);return o},r.SortedSet.prototype.toJSON=function(){return this.toArray()},r.Index=function(){this._fields=[],this._ref="id",this.pipeline=new r.Pipeline,this.documentStore=new r.Store,this.tokenStore=new r.TokenStore,this.corpusTokens=new r.SortedSet,this.eventEmitter=new r.EventEmitter,this.tokenizerFn=r.tokenizer,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},r.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},r.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},r.Index.load=function(e){e.version!==r.version&&r.utils.warn("version mismatch: current "+r.version+" importing "+e.version);var t=new this;return t._fields=e.fields,t._ref=e.ref,t.tokenizer(r.tokenizer.load(e.tokenizer)),t.documentStore=r.Store.load(e.documentStore),t.tokenStore=r.TokenStore.load(e.tokenStore),t.corpusTokens=r.SortedSet.load(e.corpusTokens),t.pipeline=r.Pipeline.load(e.pipeline),t},r.Index.prototype.field=function(e,t){var t=t||{},n={name:e,boost:t.boost||1};return this._fields.push(n),this},r.Index.prototype.ref=function(e){return this._ref=e,this},r.Index.prototype.tokenizer=function(e){var t=e.label&&e.label in r.tokenizer.registeredFunctions;return t||r.utils.warn("Function is not a registered tokenizer. This may cause problems when serialising the index"),this.tokenizerFn=e,this},r.Index.prototype.add=function(e,t){var n={},o=new r.SortedSet,s=e[this._ref],t=!(t!==void 0)||t;this._fields.forEach(function(t){var r=this.pipeline.run(this.tokenizerFn(e[t.name]));n[t.name]=r;for(var s,a=0;a<r.length;a++)s=r[a],o.add(s),this.corpusTokens.add(s)},this),this.documentStore.set(s,o);for(var a=0;a<o.length;a++){for(var i=o.elements[a],l=0,d=0;d<this._fields.length;d++){var c=this._fields[d],p=n[c.name],u=p.length;if(u){for(var g=0,f=0;f<u;f++)p[f]===i&&g++;l+=g/u*c.boost}}this.tokenStore.add(i,{ref:s,tf:l})}t&&this.eventEmitter.emit("add",e,this)},r.Index.prototype.remove=function(e,t){var n=e[this._ref],t=!(t!==void 0)||t;if(this.documentStore.has(n)){var o=this.documentStore.get(n);this.documentStore.remove(n),o.forEach(function(e){this.tokenStore.remove(e,n)},this),t&&this.eventEmitter.emit("remove",e,this)}},r.Index.prototype.update=function(e,t){var t=!(t!==void 0)||t;this.remove(e,!1),this.add(e,!1),t&&this.eventEmitter.emit("update",e,this)},r.Index.prototype.idf=function(t){var n="@"+t;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var o=this.tokenStore.count(t),r=1;return 0<o&&(r=1+e(this.documentStore.length/o)),this._idfCache[n]=r},r.Index.prototype.search=function(t){var n=this.pipeline.run(this.tokenizerFn(t)),o=new r.Vector,s=[],a=this._fields.reduce(function(e,t){return e+t.boost},0),i=n.some(function(e){return this.tokenStore.has(e)},this);if(!i)return[];n.forEach(function(t,n,i){var l=1/i.length*this._fields.length*a,d=this,c=this.tokenStore.expand(t).reduce(function(n,s){var a=d.corpusTokens.indexOf(s),c=d.idf(s),p=1,u=new r.SortedSet;if(s!==t){var g=xn(3,s.length-t.length);p=1/e(g)}-1<a&&o.insert(a,l*c*p);for(var f=d.tokenStore.get(s),h=Object.keys(f),y=h.length,_=0;_<y;_++)u.add(f[h[_]].ref);return n.union(u)},new r.SortedSet);s.push(c)},this);var l=s.reduce(function(e,t){return e.intersect(t)});return l.map(function(e){return{ref:e,score:o.similarity(this.documentVector(e))}},this).sort(function(e,t){return t.score-e.score})},r.Index.prototype.documentVector=function(e){for(var t=this.documentStore.get(e),n=t.length,o=new r.Vector,s=0;s<n;s++){var i=t.elements[s],a=this.tokenStore.get(i)[e].tf,l=this.idf(i);o.insert(this.corpusTokens.indexOf(i),a*l)}return o},r.Index.prototype.toJSON=function(){return{version:r.version,fields:this._fields,ref:this._ref,tokenizer:this.tokenizerFn.label,documentStore:this.documentStore.toJSON(),tokenStore:this.tokenStore.toJSON(),corpusTokens:this.corpusTokens.toJSON(),pipeline:this.pipeline.toJSON()}},r.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},r.Store=function(){this.store={},this.length=0},r.Store.load=function(e){var t=new this;return t.length=e.length,t.store=Object.keys(e.store).reduce(function(t,n){return t[n]=r.SortedSet.load(e.store[n]),t},{}),t},r.Store.prototype.set=function(e,t){this.has(e)||this.length++,this.store[e]=t},r.Store.prototype.get=function(e){return this.store[e]},r.Store.prototype.has=function(e){return e in this.store},r.Store.prototype.remove=function(e){this.has(e)&&(delete this.store[e],this.length--)},r.Store.prototype.toJSON=function(){return{store:this.store,length:this.length}},r.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[aeiouy]",o="[^aeiou]"+"[^aeiouy]*",r=n+"[aeiou]*",i=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*/,s=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*[aeiouy][aeiou]*[^aeiou][^aeiouy]*/,a=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*([aeiouy][aeiou]*)?$/,l=/^([^aeiou][^aeiouy]*)?[aeiouy]/,d=/^(.+?)(ss|i)es$/,c=/^(.+?)([^s])s$/,p=/^(.+?)eed$/,u=/^(.+?)(ed|ing)$/,g=/.$/,f=/(at|bl|iz)$/,h=/([^aeiouylsz])\1$/,y=/^[^aeiou][^aeiouy]*[aeiouy][^aeiouwxy]$/,_=/^(.+?[^aeiou])y$/,m=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,b=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,j=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,k=/^(.+?)(s|t)(ion)$/,x=/^(.+?)e$/,S=/ll$/,v=/^[^aeiou][^aeiouy]*[aeiouy][^aeiouwxy]$/;return function(n){var o,r,A,F,w,z,O;if(3>n.length)return n;if(A=n.substr(0,1),"y"==A&&(n=A.toUpperCase()+n.substr(1)),F=d,w=c,F.test(n)?n=n.replace(F,"$1$2"):w.test(n)&&(n=n.replace(w,"$1$2")),F=p,w=u,F.test(n)){var E=F.exec(n);F=i,F.test(E[1])&&(F=g,n=n.replace(F,""))}else if(w.test(n)){var E=w.exec(n);o=E[1],w=l,w.test(o)&&(n=o,w=f,z=h,O=y,w.test(n)?n+="e":z.test(n)?(F=g,n=n.replace(F,"")):O.test(n)&&(n+="e"))}if(F=_,F.test(n)){var E=F.exec(n);o=E[1],n=o+"i"}if(F=m,F.test(n)){var E=F.exec(n);o=E[1],r=E[2],F=i,F.test(o)&&(n=o+e[r])}if(F=b,F.test(n)){var E=F.exec(n);o=E[1],r=E[2],F=i,F.test(o)&&(n=o+t[r])}if(F=j,w=k,F.test(n)){var E=F.exec(n);o=E[1],F=s,F.test(o)&&(n=o)}else if(w.test(n)){var E=w.exec(n);o=E[1]+E[2],w=s,w.test(o)&&(n=o)}if(F=x,F.test(n)){var E=F.exec(n);o=E[1],F=s,w=a,z=v,(F.test(o)||w.test(o)&&!z.test(o))&&(n=o)}return F=S,w=s,F.test(n)&&w.test(n)&&(F=g,n=n.replace(F,"")),"y"==A&&(n=A.toLowerCase()+n.substr(1)),n}}(),r.Pipeline.registerFunction(r.stemmer,"stemmer"),r.generateStopWordFilter=function(e){var t=e.reduce(function(e,t){return e[t]=t,e},{});return function(e){if(e&&t[e]!==e)return e}},r.stopWordFilter=r.generateStopWordFilter(["a","able","about","across","after","all","almost","also","am","among","an","and","any","are","as","at","be","because","been","but","by","can","cannot","could","dear","did","do","does","either","else","ever","every","for","from","get","got","had","has","have","he","her","hers","him","his","how","however","i","if","in","into","is","it","its","just","least","let","like","likely","may","me","might","most","must","my","neither","no","nor","not","of","off","often","on","only","or","other","our","own","rather","said","say","says","she","should","since","so","some","than","that","the","their","them","then","there","these","they","this","tis","to","too","twas","us","wants","was","we","were","what","when","where","which","while","who","whom","why","will","with","would","yet","you","your"]),r.Pipeline.registerFunction(r.stopWordFilter,"stopWordFilter"),r.trimmer=function(e){return e.replace(/^\W+/,"").replace(/\W+$/,"")},r.Pipeline.registerFunction(r.trimmer,"trimmer"),r.TokenStore=function(){this.root={docs:{}},this.length=0},r.TokenStore.load=function(e){var t=new this;return t.root=e.root,t.length=e.length,t},r.TokenStore.prototype.add=function(e,t,n){var n=n||this.root,o=e.charAt(0),r=e.slice(1);return o in n||(n[o]={docs:{}}),0===r.length?(n[o].docs[t.ref]=t,void(this.length+=1)):this.add(r,t,n[o])},r.TokenStore.prototype.has=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e.charAt(n)])return!1;t=t[e.charAt(n)]}return!0},r.TokenStore.prototype.getNode=function(e){if(!e)return{};for(var t=this.root,n=0;n<e.length;n++){if(!t[e.charAt(n)])return{};t=t[e.charAt(n)]}return t},r.TokenStore.prototype.get=function(e,t){return this.getNode(e,t).docs||{}},r.TokenStore.prototype.count=function(e,t){return Object.keys(this.get(e,t)).length},r.TokenStore.prototype.remove=function(e,t){if(e){for(var n=this.root,o=0;o<e.length;o++){if(!(e.charAt(o)in n))return;n=n[e.charAt(o)]}delete n.docs[t]}},r.TokenStore.prototype.expand=function(e,t){var n=this.getNode(e),o=n.docs||{},t=t||[];return Object.keys(o).length&&t.push(e),Object.keys(n).forEach(function(n){"docs"===n||t.concat(this.expand(e+n,t))},this),t},r.TokenStore.prototype.toJSON=function(){return{root:this.root,length:this.length}},function(e,o){"function"==typeof W&&W.amd?W(o):"object"==typeof n?t.exports=o():e.lunr=o()}(this,function(){return r})})()},{}],3:[function(e,t){"use strict";var n=e("./../vendor/lodash"),o=e("lunr"),r=function(e,t){var r=this;t=t||{},t.searchableFields=t.searchableFields||[],this.items=e,this.idx=o(function(){this.field("name",{boost:10});var e=this;n.forEach(t.searchableFields,function(t){e.field(t)}),this.ref("id"),t.isExactSearch&&(this.pipeline.remove(o.stemmer),this.pipeline.remove(o.stopWordFilter))});var s=1;n.map(e,function(e){e.id||(e.id=s,++s),r.idx.add(e)}),this.store=n.mapKeys(e,function(e){return e.id})};r.prototype={search:function(e){var t=this;return e?n.map(this.idx.search(e),function(e){var n=t.store[e.ref];return n}):this.items}},t.exports=r},{"./../vendor/lodash":7,lunr:2}],4:[function(e,t){"use strict";var n=e("./../vendor/lodash");t.exports.includes=function(e,t){return!t||n.every(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.includes_any=function(e,t){return!t||t instanceof Array&&0===t.length||n.some(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.includes_any_element=function(e,t){return n.some(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.intersection=function(e,t){return t?n.intersection(e,n.flatten(t)):e};var o=function(e){try{return JSON.parse(JSON.stringify(e))}catch(t){return e}};t.exports.mergeAggregations=function(e,t){return n.mapValues(o(e),function(e,n){e.field||(e.field=n);var o=[];t.filters&&t.filters[n]&&(o=t.filters[n]),e.filters=o;var r=[];return t.not_filters&&t.not_filters[n]&&(r=t.not_filters[n]),t.exclude_filters&&t.exclude_filters[n]&&(r=t.exclude_filters[n]),e.not_filters=r,e})};t.exports.is_conjunctive_agg=function(e){return!1!==e.conjunction},t.exports.is_disjunctive_agg=function(e){return!1===e.conjunction},t.exports.is_not_filters_agg=function(e){return e.not_filters instanceof Array&&0<e.not_filters.length},t.exports.is_empty_agg=function(e){return"is_empty"===e.type},t.exports.conjunctive_field=function(e,n){return t.exports.includes(e,n)},t.exports.disjunctive_field=function(e,n){return t.exports.includes_any(e,n)},t.exports.not_filters_field=function(e,n){return!t.exports.includes_any_element(e,n)},t.exports.check_empty_field=function(e,n){var o=["not_empty"];return(""===e||void 0===e||null===e||e instanceof Array&&0===e.length)&&(o=["empty"]),n&&!t.exports.includes(o,n)?!1:o}},{"./../vendor/lodash":7}],5:[function(e,t){"use strict";var n=e("./lib"),o=e("./../vendor/lodash"),r=e("./helpers"),i=e("./fulltext");t.exports=function(e,t){t=t||{};var o=new i(e,t);return{search:function(i){return i=i||{},i.aggregations=r.mergeAggregations(t.aggregations,i),n.search(e,i,t,o)},similar:function(t,o){return n.similar(e,t,o)},aggregation:function(o){return n.aggregation(e,o,t.aggregations)},reindex:function(n){e=n,o=new i(e,t)}}}},{"./../vendor/lodash":7,"./fulltext":3,"./helpers":4,"./lib":6}],6:[function(e,t){"use strict";var n=e("./../vendor/lodash"),o=e("./helpers"),r=e("./fulltext");t.exports.search=function(e,n,o,r){n=n||{};var i=0;if(r){var s=new Date().getTime();e=r.search(n.query),i=new Date().getTime()-s}n.filter instanceof Function&&(e=e.filter(n.filter)),n.prefilter instanceof Function&&(e=n.prefilter(e));var a=t.exports.items_by_aggregations(e,n.aggregations),l=n.per_page||12,d=n.page||1,c=0;if(n.sort){var p=new Date().getTime();a=t.exports.sorted_items(a,n.sort,o.sortings),c=new Date().getTime()-p}var u=new Date().getTime(),g=t.exports.aggregations(e,n.aggregations),f=new Date().getTime()-u;return{pagination:{per_page:l,page:d,total:a.length},timings:{facets:f,search:i,sorting:c},data:{items:a.slice((d-1)*l,d*l),aggregations:g}}},t.exports.aggregation=function(e,o,r){var i=o.per_page||10,s=o.page||1;if(o.name&&(!r||!r[o.name]))throw new Error("Please define aggregation \"".concat(o.name,"\" in config"));var a=t.exports.buckets(e,o.name,r[o.name],r);return o.query&&(a=n.filter(a,function(e){return 0===e.key.toLowerCase().indexOf(o.query.toLowerCase())})),{pagination:{per_page:i,page:s,total:a.length},data:{buckets:a.slice((s-1)*i,s*i)}}},t.exports.sorted_items=function(e,t,o){return o&&o[t]&&(t=o[t]),t.field?n.orderBy(e,t.field,t.order||"asc"):e},t.exports.items_by_aggregations=function(e,o){return n.filter(e,function(e){return t.exports.filterable_item(e,o)})},t.exports.aggregations=function(e,o){var r=0;return n.mapValues(o,function(n,i){return++r,{name:i,title:n.title||i.charAt(0).toUpperCase()+i.slice(1),position:r,buckets:t.exports.buckets(e,i,n,o).slice(0,n.size||10)}})},t.exports.filterable_item=function(e,t){for(var r,s=n.keys(t),a=0;a<s.length;++a){if(r=s[a],o.is_empty_agg(t[r])){if(o.check_empty_field(e[t[r].field],t[r].filters))continue;return!1}if(o.is_not_filters_agg(t[r])&&!o.not_filters_field(e[r],t[r].not_filters))return!1;if(o.is_disjunctive_agg(t[r])&&!o.disjunctive_field(e[r],t[r].filters))return!1;if(o.is_conjunctive_agg(t[r])&&!o.conjunctive_field(e[r],t[r].filters))return!1}return!0},t.exports.bucket_field=function(e,t,r){for(var s,a=n.keys(t),l=0;l<a.length;++l)if(s=a[l],o.is_not_filters_agg(t[s])&&!o.not_filters_field(e[s],t[s].not_filters))return[];for(var l=0;l<a.length;++l)if(a[l]!==r){var s=a[l];if(o.is_empty_agg(t[s])){if(!o.check_empty_field(e[t[s].field],t[s].filters))return[];continue}else{if(o.is_disjunctive_agg(t[s])&&!o.disjunctive_field(e[s],t[s].filters))return[];if(o.is_conjunctive_agg(t[s])&&!o.conjunctive_field(e[s],t[s].filters))return[]}}if(o.is_empty_agg(t[r])){var i=o.check_empty_field(e[t[r].field],t[r].filters);return i?i:[]}return o.is_disjunctive_agg(t[r])||o.includes(e[r],t[r].filters)?e[r]?n.flatten([e[r]]):[]:[]},t.exports.bucket=function(e,o){return n.mapValues(o,function(n,r){return t.exports.bucket_field(e,o,r)})},t.exports.buckets=function(e,r,s,a){var i=n.transform(e,function(e,n){n=t.exports.bucket(n,a);var l=n[r];if(!1!==s.conjunction&&o.includes(l,s.filters)||!1===s.conjunction)for(var d,c=0;l&&c<l.length;++c)d=l[c],e[d]?e[d]+=1:e[d]=1},{});return i=n.map(i,function(e,t){return{key:t,doc_count:e,selected:n.includes(s.filters,t)}}),i="term"===s.sort?n.orderBy(i,["selected","key"],["desc",s.order||"asc"]):n.orderBy(i,["selected","doc_count","key"],["desc",s.order||"desc","asc"]),i},t.exports.similar=function(e,t,o){for(var r,s=o.per_page||10,a=o.minimum||0,l=o.page||1,d=0;d<e.length;++d)if(e[d].id==t){r=e[d];break}if(!o.field)throw new Error("Please define field in options");for(var i=o.field,c=[],d=0;d<e.length;++d)if(e[d].id!==t){var p=n.intersection(r[i],e[d][i]);p.length>=a&&(c.push(e[d]),c[c.length-1].intersection_length=p.length)}return c=n.orderBy(c,["intersection_length"],["desc"]),{pagination:{per_page:s,page:l,total:c.length},data:{items:c.slice((l-1)*s,l*s)}}}},{"./../vendor/lodash":7,"./fulltext":3,"./helpers":4}],7:[function(e,Fn,zn){(function(En){"use strict";function Tn(e){return Tn="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},Tn(e)}(function(){function In(n,o){return n.set(o[0],o[1]),n}function t(n,t){return n.add(t),n}function Pn(o,t,e){switch(e.length){case 0:return o.call(t);case 1:return o.call(t,e[0]);case 2:return o.call(t,e[0],e[1]);case 3:return o.call(t,e[0],e[1],e[2]);}return o.apply(t,e)}function Nn(o,t){for(var e=-1,n=null==o?0:o.length;++e<n&&!1!==t(o[e],e,o););return o}function Vn(o,t){for(var e=-1,n=null==o?0:o.length;++e<n;)if(!t(o[e],e,o))return!1;return!0}function n(i,t){for(var e,s=-1,n=null==i?0:i.length,r=0,o=[];++s<n;)e=i[s],t(e,s,i)&&(o[r++]=e);return o}function u(n,t){return null!=n&&n.length&&-1<(t===t?y(n,t,0):o(n,s,0))}function c(i,t){for(var e=-1,n=null==i?0:i.length,r=Array(n);++e<n;)r[e]=t(i[e],e,i);return r}function Cn(i,t){for(var e=-1,n=t.length,r=i.length;++e<n;)i[r+e]=t[e];return i}function e(i,t,e){for(var n=-1,r=null==i?0:i.length;++n<r;)e=t(e,i[n],n,i);return e}function i(o,t){for(var e=-1,n=null==o?0:o.length;++e<n;)if(t(o[e],e,o))return!0;return!1}function o(o,t,e){var n=o.length;for(e+=-1;++e<n;)if(t(o[e],e,o))return e;return-1}function s(e){return e!==e}function r(n){return function(t){return null==t?Kt:t[n]}}function a(o,t){var e=o.length;for(o.sort(t);e--;)o[e]=o[e].c;return o}function l(n){return function(t){return n(t)}}function p(n,t){return c(t,function(t){return n[t]})}function f(o){var i=-1,e=Array(o.size);return o.forEach(function(n,t){e[++i]=[t,n]}),e}function h(o){var t=Object;return function(e){return o(t(e))}}function g(o){var r=-1,e=Array(o.size);return o.forEach(function(n){e[++r]=n}),e}function y(o,t,e){--e;for(var n=o.length;++e<n;)if(o[e]===t)return e;return-1}function d(){}function _(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function m(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function b(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function j(o){var t=-1,e=null==o?0:o.length;for(this.__data__=new b;++t<e;)this.add(o[t])}function S(e){this.size=(this.__data__=new m(e)).size}function k(s,t){var l=vn(s),d=!l&&mn(s),p=!l&&!d&&An(s),g=!l&&!d&&!p&&Sn(s);if(l=l||d||p||g){for(var d=s.length,u=String,c=-1,i=Array(d);++c<d;)i[c]=u(c);d=i}else d=[];var a,u=d.length;for(a in s)!t&&!Fe.call(s,a)||l&&("length"==a||p&&("offset"==a||"parent"==a)||g&&("buffer"==a||"byteLength"==a||"byteOffset"==a)||gt(a,u))||d.push(a);return d}function v(o,t,e){var n=o[t];Fe.call(o,t)&&xt(n,e)&&(e!==Kt||t in o)||w(o,t,e)}function x(o,t){for(var e=o.length;e--;)if(xt(o[e][0],t))return e;return-1}function A(n,t){return n&&et(t,Tt(t),n)}function F(n,t){return n&&et(t,Mt(t),n)}function w(o,t,e){"__proto__"==t&&Te?Te(o,t,{configurable:!0,enumerable:!0,value:e,writable:!0}):o[t]=e}function z(l,t,e,n,o,d){var c,r=1&t,i=2&t;if(e&&(c=o?e(l,n,o,d):e(l)),c!==Kt)return c;if(!Ot(l))return l;if(!(n=vn(l))){var p=bn(l),s="[object Function]"==p||"[object GeneratorFunction]"==p;if(An(l))return X(l,r);if("[object Object]"!=p&&"[object Arguments]"!=p&&(!s||o)){if(!pe[p])return o?l:{};c=ft(l,p,z,r)}else if(c=i||s?{}:"function"!=typeof l.constructor||_t(l)?{}:sn(Le(l)),!r)return i?nt(l,F(c,l)):tt(l,A(c,l))}else if(c=ut(l),!r)return Z(l,c);if(d||(d=new S),o=d.get(l))return o;d.set(l,c);var i=4&t?i?at:it:i?Mt:Tt,u=n?Kt:i(l);return Nn(u||l,function(n,r){u&&(r=n,n=l[r]),v(c,r,z(n,t,e,r,l,d))}),c}function O(o,i){var e=!0;return un(o,function(n,t,r){return e=!!i(n,t,r)}),e}function E(o,i){var e=[];return un(o,function(n,t,r){i(n,t,r)&&e.push(n)}),e}function I(s,t,e,n,r){var o=-1,a=s.length;for(e||(e=ht),r||(r=[]);++o<a;){var l=s[o];0<t&&e(l)?1<t?I(l,t-1,e,n,r):Cn(r,l):n||(r[r.length]=l)}return r}function U(n,t){return n&&fn(n,t,Tt)}function L(o,t){t=Q(t,o);for(var e=0,n=t.length;null!=o&&e<n;)o=o[jt(t[e++])];return e&&e==n?o:Kt}function P(o,t,e){return t=t(o),vn(o)?t:Cn(t,e(o))}function N(r){if(null==r)r=r===Kt?"[object Undefined]":"[object Null]";else if(Re&&Re in Object(r)){var t=Fe.call(r,Re),e=r[Re];try{r[Re]=Kt}catch(e){}var n=Ee.call(r);t?r[Re]=e:delete r[Re],r=n}else r=Ee.call(r);return r}function V(e){return Et(e)&&"[object Arguments]"==N(e)}function C(d,t,e,g,y){if(d===t)t=!0;else if(null==d||null==t||!Et(d)&&!Et(t))t=d!==d&&t!==t;else t:{var o=vn(d),u=vn(t),c=o?"[object Array]":bn(d),i=u?"[object Array]":bn(t),c="[object Arguments]"==c?"[object Object]":c,i="[object Arguments]"==i?"[object Object]":i,a="[object Object]"==c,u="[object Object]"==i;if((i=c==i)&&An(d)){if(!An(t)){t=!1;break t}o=!0,a=!1}if(i&&!a)y||(y=new S),t=o||Sn(d)?ot(d,t,e,g,C,y):rt(d,t,c,e,g,C,y);else{if(!(1&e)&&(o=a&&Fe.call(d,"__wrapped__"),c=u&&Fe.call(t,"__wrapped__"),o||c)){d=o?d.value():d,t=c?t.value():t,y||(y=new S),t=C(d,t,e,g,y);break t}if(i){e:if(y||(y=new S),o=1&e,c=it(d),u=c.length,i=it(t).length,u==i||o){for(a=u;a--;){var _=c[a];if(o?!(_ in t):!Fe.call(t,_)){t=!1;break e}}if((i=y.get(d))&&y.get(t))t=i==t;else{i=!0,y.set(d,t),y.set(t,d);for(var l=o;++a<u;){var _=c[a],m=d[_],b=t[_];if(g)var h=o?g(b,m,_,t,d,y):g(m,b,_,d,t,y);if(h===Kt?m!==b&&!C(m,b,e,g,y):!h){i=!1;break}l||(l="constructor"==_)}i&&!l&&(e=d.constructor,g=t.constructor,e!=g&&"constructor"in d&&"constructor"in t&&!("function"==typeof e&&e instanceof e&&"function"==typeof g&&g instanceof g)&&(i=!1)),y.delete(d),y.delete(t),t=i}}else t=!1;}else t=!1}}return t}function B(s,t){var e=t.length,n=e;if(null==s)return!n;for(s=Object(s);e--;){var a=t[e];if(a[2]?a[1]!==s[a[0]]:!(a[0]in s))return!1}for(;++e<n;){var a=t[e],o=a[0],l=s[o],d=a[1];if(a[2]){if(l===Kt&&!(o in s))return!1;}else if(a=new S,void 0===Kt?!C(d,l,3,void 0,a):1)return!1}return!0}function T(e){return"function"==typeof e?e:null==e?Wt:"object"==Tn(e)?vn(e)?D(e[0],e[1]):R(e):Jt(e)}function M(o,i){var e=-1,n=At(o)?Array(o.length):[];return un(o,function(r,t,o){n[++e]=i(r,t,o)}),n}function R(o){var t=ct(o);return 1==t.length&&t[0][2]?mt(t[0][0],t[0][1]):function(e){return e===o||B(e,t)}}function D(o,t){return dt(o)&&t===t&&!Ot(t)?mt(jt(o),t):function(e){var n=Ct(e,o);return n===Kt&&n===t?Bt(e,o):C(t,n,3)}}function $(o,i,d){var e=-1;return i=c(i.length?i:[Wt],l(st())),o=M(o,function(n){return{a:c(i,function(t){return t(n)}),b:++e,c:n}}),a(o,function(n,t){var e;t:{e=-1;for(var r=n.a,o=t.a,u=r.length,c=d.length;++e<u;){var i;e:{i=r[e];var a=o[e];if(i!==a){var g=i!==Kt,f=null===i,_=i===i,m=Ut(i),k=a!==Kt,x=null===a,S=a===a,A=Ut(a);if(!x&&!A&&!m&&i>a||m&&k&&S&&!x&&!A||f&&k&&S||!g&&S||!_){i=1;break e}if(!f&&!m&&!A&&i<a||A&&g&&_&&!f&&!m||x&&g&&_||!k&&_||!S){i=-1;break e}}i=0}if(i){e=e>=c?i:i*("desc"==d[e]?-1:1);break t}}e=n.b-t.b}return e})}function J(n){return function(t){return L(t,n)}}function q(e){return gn(bt(e,Wt),e+"")}function H(o,i){var e;return un(o,function(s,t,r){return e=i(s,t,r),!e}),!!e}function K(n){if("string"==typeof n)return n;if(vn(n))return c(n,K)+"";if(Ut(n))return ln?ln.call(n):"";var o=n+"";return"0"==o&&1/n==-Gt?"-0":o}function G(e){return Ft(e)?e:[]}function Q(n,t){return vn(n)?n:dt(n,t)?[n]:_n(Vt(n))}function X(o,t){if(t)return o.slice();var e=o.length,e=De?De(e):new o.constructor(e);return o.copy(e),e}function Y(n){var t=new n.constructor(n.byteLength);return new Ue(t).set(new Ue(n)),t}function Z(o,t){var e=-1,n=o.length;for(t||(t=Array(n));++e<n;)t[e]=o[e];return t}function et(s,t,e){var a=!e;e||(e={});for(var r=-1,o=t.length;++r<o;){var l=t[r],d=Kt;d===Kt&&(d=s[l]),a?w(e,l,d):v(e,l,d)}return e}function tt(n,t){return et(n,hn(n),t)}function nt(n,t){return et(n,yn(n),t)}function ot(l,t,d,n,r,o){var e=1&d,c=l.length,u=t.length;if(c!=u&&!(e&&u>c))return!1;if((u=o.get(l))&&o.get(t))return u==t;var u=-1,a=!0,g=2&d?new j:Kt;for(o.set(l,t),o.set(t,l);++u<c;){var s=l[u],f=t[u];if(n)var h=e?n(f,s,u,t,l,o):n(s,f,u,l,t,o);if(h!==Kt){if(h)continue;a=!1;break}if(g){if(!i(t,function(i,t){if(!g.has(t)&&(s===i||r(s,i,d,n,o)))return g.push(t)})){a=!1;break}}else if(s!==f&&!r(s,f,d,n,o)){a=!1;break}}return o.delete(l),o.delete(t),a}function rt(s,a,l,n,r,o,d){switch(l){case"[object DataView]":if(s.byteLength!=a.byteLength||s.byteOffset!=a.byteOffset)break;s=s.buffer,a=a.buffer;case"[object ArrayBuffer]":if(s.byteLength!=a.byteLength||!o(new Ue(s),new Ue(a)))break;return!0;case"[object Boolean]":case"[object Date]":case"[object Number]":return xt(+s,+a);case"[object Error]":return s.name==a.name&&s.message==a.message;case"[object RegExp]":case"[object String]":return s==a+"";case"[object Map]":var c=f;case"[object Set]":if(c||(c=g),s.size!=a.size&&!(1&n))break;return(l=d.get(s))?l==a:(n|=2,d.set(s,a),a=ot(c(s),c(a),n,r,o,d),d.delete(s),a);case"[object Symbol]":if(cn)return cn.call(s)==cn.call(a);}return!1}function it(e){return P(e,Tt,hn)}function at(e){return P(e,Mt,yn)}function st(){var e=d.iteratee||$t,e=e===$t?T:e;return arguments.length?e(arguments[0],arguments[1]):e}function lt(o,t){var i=o.__data__,n=Tn(t);return("string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==t:null===t)?i["string"==typeof t?"string":"hash"]:i.map}function ct(i){for(var t=Tt(i),e=t.length;e--;){var n=t[e],r=i[n];t[e]=[n,r,r===r&&!Ot(r)]}return t}function pt(o,t){var e=null==o?Kt:o[t];return(!Ot(e)||Ie&&Ie in e?0:(wt(e)?$e:le).test(St(e)))?e:Kt}function ut(o){var t=o.length,e=o.constructor(t);return t&&"string"==typeof o[0]&&Fe.call(o,"index")&&(e.index=o.index,e.input=o.input),e}function ft(i,s,r,o){var a=i.constructor;return"[object ArrayBuffer]"===s?Y(i):"[object Boolean]"===s||"[object Date]"===s?new a(+i):"[object DataView]"===s?(s=o?Y(i.buffer):i.buffer,new i.constructor(s,i.byteOffset,i.byteLength)):"[object Float32Array]"===s||"[object Float64Array]"===s||"[object Int8Array]"===s||"[object Int16Array]"===s||"[object Int32Array]"===s||"[object Uint8Array]"===s||"[object Uint8ClampedArray]"===s||"[object Uint16Array]"===s||"[object Uint32Array]"===s?(s=o?Y(i.buffer):i.buffer,new i.constructor(s,i.byteOffset,i.length)):"[object Map]"===s?(s=o?r(f(i),1):f(i),e(s,In,new i.constructor)):"[object Number]"===s||"[object String]"===s?new a(i):"[object RegExp]"===s?(s=new i.constructor(i.source,ie.exec(i)),s.lastIndex=i.lastIndex,s):"[object Set]"===s?(s=o?r(g(i),1):g(i),e(s,t,new i.constructor)):"[object Symbol]"===s?cn?Object(cn.call(i)):{}:void 0}function ht(e){return vn(e)||mn(e)||!!(Ce&&e&&e[Ce])}function gt(n,o){return o=null==o?9007199254740991:o,!!o&&("number"==typeof n||be.test(n))&&-1<n&&0==n%1&&n<o}function yt(o,t,e){if(!Ot(e))return!1;var n=Tn(t);return!("number"==n?!(At(e)&&gt(t,e.length)):!("string"==n&&t in e))&&xt(e[t],o)}function dt(o,t){if(vn(o))return!1;var e=Tn(o);return"number"==e||"symbol"==e||"boolean"==e||null==o||Ut(o)||ne.test(o)||!ee.test(o)||null!=t&&o in Object(t)}function _t(n){var t=n&&n.constructor;return n===("function"==typeof t&&t.prototype||ke)}function mt(o,t){return function(e){return null!=e&&e[o]===t&&(t!==Kt||o in Object(e))}}function bt(n,t){var e,e=Ke(e===Kt?n.length-1:e,0);return function(){for(var r=arguments,o=-1,s=Ke(r.length-e,0),a=Array(s);++o<s;)a[o]=r[e+o];for(o=-1,s=Array(e+1);++o<e;)s[o]=r[o];return s[e]=t(a),Pn(n,this,s)}}function jt(n){if("string"==typeof n||Ut(n))return n;var o=n+"";return"0"==o&&1/n==-Gt?"-0":o}function St(e){if(null!=e){try{return xe.call(e)}catch(e){}return e+""}return""}function kt(n,t){return(vn(n)?Nn:un)(n,st(t,3))}function vt(i,s){function a(){var e=arguments,t=s?s.apply(this,e):e[0],n=a.cache;return n.has(t)?n.get(t):(e=i.apply(this,e),a.cache=n.set(t,e)||n,e)}if("function"!=typeof i||null!=s&&"function"!=typeof s)throw new TypeError("Expected a function");return a.cache=new(vt.Cache||b),a}function xt(n,t){return n===t||n!==n&&t!==t}function At(e){return null!=e&&zt(e.length)&&!wt(e)}function Ft(e){return Et(e)&&At(e)}function wt(e){return!!Ot(e)&&(e=N(e),"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e)}function zt(e){return"number"==typeof e&&-1<e&&0==e%1&&9007199254740991>=e}function Ot(n){var t=Tn(n);return null!=n&&("object"==t||"function"==t)}function Et(e){return null!=e&&"object"==Tn(e)}function It(e){return"string"==typeof e||!vn(e)&&Et(e)&&"[object String]"==N(e)}function Ut(e){return"symbol"==Tn(e)||Et(e)&&"[object Symbol]"==N(e)}function Lt(e){return e?(e=Nt(e),e===Gt||e===-Gt?1.7976931348623157e308*(0>e?-1:1):e===e?e:0):0===e?e:0}function Pt(n){n=Lt(n);var t=n%1;return n===n?t?n-t:n:0}function Nt(n){if("number"==typeof n)return n;if(Ut(n))return Qt;if(Ot(n)&&(n="function"==typeof n.valueOf?n.valueOf():n,n=Ot(n)?n+"":n),"string"!=typeof n)return 0===n?n:+n;n=n.replace(ue,"");var o=fe.test(n);return o||se.test(n)?je(n.slice(2),o?2:8):ae.test(n)?Qt:+n}function Vt(e){return null==e?"":K(e)}function Ct(o,t,e){return o=null==o?Kt:L(o,t),o===Kt?e:o}function Bt(s,t){var e;if(e=null!=s){e=s;var n=Q(t,e);for(var r,i=-1,o=n.length,a=!1;++i<o&&(r=jt(n[i]),!!(a=null!=e&&null!=e&&r in Object(e)));)e=e[r];a||++i!=o?e=a:(o=null==e?0:e.length,e=!!o&&zt(o)&&gt(r,o)&&(vn(e)||mn(e)))}return e}function Tt(o){if(At(o))o=k(o);else if(_t(o)){var t,e=[];for(t in Object(o))Fe.call(o,t)&&"constructor"!=t&&e.push(t);o=e}else o=qe(o);return o}function Mt(o){if(At(o))o=k(o,!0);else if(Ot(o)){var t,e=_t(o),i=[];for(t in o)("constructor"!=t||!e&&Fe.call(o,t))&&i.push(t);o=i}else{if(t=[],null!=o)for(e in Object(o))t.push(e);o=t}return o}function Rt(e){return null==e?[]:p(e,Tt(e))}function Dt(e){return function(){return e}}function Wt(e){return e}function $t(e){return T("function"==typeof e?e:z(e,1))}function Jt(e){return dt(e)?r(jt(e)):J(e)}function qt(){return[]}function Ht(){return!1}var Kt,Gt=1/0,Qt=NaN,ee=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,ne=/^\w*$/,re=/^\./,oe=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,ue=/^\s+|\s+$/g,ce=/\\(\\)?/g,ie=/\w*$/,ae=/^[-+]0x[0-9a-f]+$/i,fe=/^0b[01]+$/i,le=/^\[object .+?Constructor\]$/,se=/^0o[0-7]+$/i,be=/^(?:0|[1-9]\d*)$/,he={};he["[object Float32Array]"]=he["[object Float64Array]"]=he["[object Int8Array]"]=he["[object Int16Array]"]=he["[object Int32Array]"]=he["[object Uint8Array]"]=he["[object Uint8ClampedArray]"]=he["[object Uint16Array]"]=he["[object Uint32Array]"]=!0,he["[object Arguments]"]=he["[object Array]"]=he["[object ArrayBuffer]"]=he["[object Boolean]"]=he["[object DataView]"]=he["[object Date]"]=he["[object Error]"]=he["[object Function]"]=he["[object Map]"]=he["[object Number]"]=he["[object Object]"]=he["[object RegExp]"]=he["[object Set]"]=he["[object String]"]=he["[object WeakMap]"]=!1;var pe={};pe["[object Arguments]"]=pe["[object Array]"]=pe["[object ArrayBuffer]"]=pe["[object DataView]"]=pe["[object Boolean]"]=pe["[object Date]"]=pe["[object Float32Array]"]=pe["[object Float64Array]"]=pe["[object Int8Array]"]=pe["[object Int16Array]"]=pe["[object Int32Array]"]=pe["[object Map]"]=pe["[object Number]"]=pe["[object Object]"]=pe["[object RegExp]"]=pe["[object Set]"]=pe["[object String]"]=pe["[object Symbol]"]=pe["[object Uint8Array]"]=pe["[object Uint8ClampedArray]"]=pe["[object Uint16Array]"]=pe["[object Uint32Array]"]=!0,pe["[object Error]"]=pe["[object Function]"]=pe["[object WeakMap]"]=!1;var ye,je=parseInt,ve="object"==("undefined"==typeof En?"undefined":Tn(En))&&En&&En.Object===Object&&En,ge="object"==("undefined"==typeof self?"undefined":Tn(self))&&self&&self.Object===Object&&self,_e=ve||ge||Function("return this")(),de="object"==("undefined"==typeof zn?"undefined":Tn(zn))&&zn&&!zn.nodeType&&zn,Ae=de&&"object"==("undefined"==typeof Fn?"undefined":Tn(Fn))&&Fn&&!Fn.nodeType&&Fn,me=Ae&&Ae.exports===de,we=me&&ve.process;t:{try{ye=we&&we.binding&&we.binding("util");break t}catch(e){}ye=void 0}var Oe=ye&&ye.isTypedArray,Se=Array.prototype,ke=Object.prototype,ze=_e["__core-js_shared__"],xe=Function.prototype.toString,Fe=ke.hasOwnProperty,Ie=function(){var e=/[^.]+$/.exec(ze&&ze.keys&&ze.keys.IE_PROTO||"");return e?"Symbol(src)_1."+e:""}(),Ee=ke.toString,$e=RegExp("^"+xe.call(Fe).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$"),Be=me?_e.Buffer:Kt,Me=_e.Symbol,Ue=_e.Uint8Array,De=Be?Be.f:Kt,Le=h(Object.getPrototypeOf),Pe=Object.create,Ne=ke.propertyIsEnumerable,Ve=Se.splice,Ce=Me?Me.isConcatSpreadable:Kt,Re=Me?Me.toStringTag:Kt,Te=function(){try{var e=pt(Object,"defineProperty");return e({},"",{}),e}catch(e){}}(),We=Object.getOwnPropertySymbols,Ge=Be?Be.isBuffer:Kt,qe=h(Object.keys),Ke=xn,He=Math.min,Je=Date.now,Qe=pt(_e,"DataView"),Xe=pt(_e,"Map"),Xt=pt(_e,"Promise"),Ze=pt(_e,"Set"),Yt=pt(_e,"WeakMap"),Zt=pt(Object,"create"),en=St(Qe),tn=St(Xe),nn=St(Xt),on=St(Ze),rn=St(Yt),an=Me?Me.prototype:Kt,cn=an?an.valueOf:Kt,ln=an?an.toString:Kt,sn=function(){function n(){}return function(t){return Ot(t)?Pe?Pe(t):(n.prototype=t,t=new n,n.prototype=Kt,t):{}}}();_.prototype.clear=function(){this.__data__=Zt?Zt(null):{},this.size=0},_.prototype.delete=function(e){return e=this.has(e)&&delete this.__data__[e],this.size-=e?1:0,e},_.prototype.get=function(n){var t=this.__data__;return Zt?(n=t[n],"__lodash_hash_undefined__"===n?Kt:n):Fe.call(t,n)?t[n]:Kt},_.prototype.has=function(n){var t=this.__data__;return Zt?t[n]!==Kt:Fe.call(t,n)},_.prototype.set=function(o,t){var e=this.__data__;return this.size+=this.has(o)?0:1,e[o]=Zt&&t===Kt?"__lodash_hash_undefined__":t,this},m.prototype.clear=function(){this.__data__=[],this.size=0},m.prototype.delete=function(n){var t=this.__data__;return n=x(t,n),!(0>n)&&(n==t.length-1?t.pop():Ve.call(t,n,1),--this.size,!0)},m.prototype.get=function(n){var t=this.__data__;return n=x(t,n),0>n?Kt:t[n][1]},m.prototype.has=function(e){return-1<x(this.__data__,e)},m.prototype.set=function(o,t){var e=this.__data__,n=x(e,o);return 0>n?(++this.size,e.push([o,t])):e[n][1]=t,this},b.prototype.clear=function(){this.size=0,this.__data__={hash:new _,map:new(Xe||m),string:new _}},b.prototype.delete=function(e){return e=lt(this,e).delete(e),this.size-=e?1:0,e},b.prototype.get=function(e){return lt(this,e).get(e)},b.prototype.has=function(e){return lt(this,e).has(e)},b.prototype.set=function(o,t){var e=lt(this,o),n=e.size;return e.set(o,t),this.size+=e.size==n?0:1,this},j.prototype.add=j.prototype.push=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this},j.prototype.has=function(e){return this.__data__.has(e)},S.prototype.clear=function(){this.__data__=new m,this.size=0},S.prototype.delete=function(n){var t=this.__data__;return n=t.delete(n),this.size=t.size,n},S.prototype.get=function(e){return this.__data__.get(e)},S.prototype.has=function(e){return this.__data__.has(e)},S.prototype.set=function(o,t){var e=this.__data__;if(e instanceof m){var n=e.__data__;if(!Xe||199>n.length)return n.push([o,t]),this.size=++e.size,this;e=this.__data__=new b(n)}return e.set(o,t),this.size=e.size,this};var un=function(i,t){return function(e,n){if(null==e)return e;if(!At(e))return i(e,n);for(var r=e.length,o=t?r:-1,s=Object(e);(t?o--:++o<r)&&!1!==n(s[o],o,s););return e}}(U),fn=function(e){return function(t,s,n){var r=-1,o=Object(t);n=n(t);for(var a,i=n.length;i--&&(a=n[e?i:++r],!1!==s(o[a],a,o)););return t}}(),pn=Te?function(n,t){return Te(n,"toString",{configurable:!0,enumerable:!1,value:Dt(t),writable:!0})}:Wt,hn=We?function(o){return null==o?[]:(o=Object(o),n(We(o),function(t){return Ne.call(o,t)}))}:qt,yn=We?function(n){for(var t=[];n;)Cn(t,hn(n)),n=Le(n);return t}:qt,bn=N;(Qe&&"[object DataView]"!=bn(new Qe(new ArrayBuffer(1)))||Xe&&"[object Map]"!=bn(new Xe)||Xt&&"[object Promise]"!=bn(Xt.resolve())||Ze&&"[object Set]"!=bn(new Ze)||Yt&&"[object WeakMap]"!=bn(new Yt))&&(bn=function(n){var t=N(n);if(n=(n="[object Object]"==t?n.constructor:Kt)?St(n):"")switch(n){case en:return"[object DataView]";case tn:return"[object Map]";case nn:return"[object Promise]";case on:return"[object Set]";case rn:return"[object WeakMap]";}return t});var gn=function(i){var t=0,e=0;return function(){var n=Je(),r=16-(n-e);if(!(e=n,0<r))t=0;else if(800<=++t)return arguments[0];return i.apply(Kt,arguments)}}(pn),_n=function(n){n=vt(n,function(e){return 500===o.size&&o.clear(),e});var o=n.cache;return n}(function(n){var i=[];return re.test(n)&&i.push(""),n.replace(oe,function(e,t,n,r){i.push(n?r.replace(ce,"$1"):t||e)}),i}),dn=q(function(i){var t=c(i,G);if(t.length&&t[0]===i[0]){i=t[0].length;for(var e=t.length,n=e,r=Array(e),o=1/0,d=[];n--;){var a=t[n],o=He(a.length,o);r[n]=120<=i&&120<=a.length?new j(n&&a):Kt}var a=t[0],g=-1,l=r[0];t:for(;++g<i&&d.length<o;){var s=a[g],f=s,s=0===s?0:s;if(l?!l.has(f):!u(d,f)){for(n=e;--n;){var h=r[n];if(h?!h.has(f):!u(t[n],f))continue t}l&&l.push(f),d.push(s)}}t=d}else t=[];return t}),jn=q(function(o,t){if(null==o)return[];var e=t.length;return 1<e&&yt(o,t[0],t[1])?t=[]:2<e&&yt(t[0],t[1],t[2])&&(t=[t[0]]),$(o,I(t,1),[])});vt.Cache=b;var mn=V(function(){return arguments}())?V:function(e){return Et(e)&&Fe.call(e,"callee")&&!Ne.call(e,"callee")},vn=Array.isArray,An=Ge||Ht,Sn=Oe?l(Oe):function(e){return Et(e)&&zt(e.length)&&!!he[N(e)]};d.constant=Dt,d.filter=function(o,t){return(vn(o)?n:E)(o,st(t,3))},d.flatten=function(e){return(null==e?0:e.length)?I(e,1):[]},d.intersection=dn,d.iteratee=$t,d.keys=Tt,d.keysIn=Mt,d.map=function(n,t){return(vn(n)?c:M)(n,st(t,3))},d.mapKeys=function(o,i){var e={};return i=st(i,3),U(o,function(n,t,r){w(e,i(n,t,r),n)}),e},d.mapValues=function(o,i){var e={};return i=st(i,3),U(o,function(n,t,r){w(e,t,i(n,t,r))}),e},d.memoize=vt,d.orderBy=function(o,t,e,n){return null==o?[]:(vn(t)||(t=null==t?[]:[t]),e=n?Kt:e,vn(e)||(e=null==e?[]:[e]),$(o,t,e))},d.property=Jt,d.sortBy=jn,d.transform=function(r,i,e){var t=vn(r),n=t||An(r)||Sn(r);if(i=st(i,4),null==e){var o=r&&r.constructor;e=n?t?new o:[]:Ot(r)&&wt(o)?sn(Le(r)):{}}return(n?Nn:U)(r,function(n,t,r){return i(e,n,t,r)}),e},d.values=Rt,d.clone=function(e){return z(e,4)},d.eq=xt,d.every=function(o,t,e){var n=vn(o)?Vn:O;return e&&yt(o,t,e)&&(t=Kt),n(o,st(t,3))},d.forEach=kt,d.get=Ct,d.hasIn=Bt,d.identity=Wt,d.includes=function(i,t,e,n){return i=At(i)?i:Rt(i),e=e&&!n?Pt(e):0,n=i.length,0>e&&(e=Ke(n+e,0)),It(i)?e<=n&&-1<i.indexOf(t,e):!!n&&-1<(t===t?y(i,t,e):o(i,s,e))},d.isArguments=mn,d.isArray=vn,d.isArrayLike=At,d.isArrayLikeObject=Ft,d.isBuffer=An,d.isFunction=wt,d.isLength=zt,d.isObject=Ot,d.isObjectLike=Et,d.isString=It,d.isSymbol=Ut,d.isTypedArray=Sn,d.stubArray=qt,d.stubFalse=Ht,d.some=function(o,t,e){var n=vn(o)?i:H;return e&&yt(o,t,e)&&(t=Kt),n(o,st(t,3))},d.toFinite=Lt,d.toInteger=Pt,d.toNumber=Nt,d.toString=Vt,d.each=kt,d.VERSION="4.17.4","function"==typeof W&&"object"==Tn(W.amd)&&W.amd?(_e._=d,W(function(){return d})):Ae?((Ae.exports=d)._=d,de._=d):_e._=d}).call(void 0)}).call(this,"undefined"==typeof global?"undefined"==typeof self?"undefined"==typeof window?{}:window:self:global)},{}]},{},[1])(1)});
  (function() {
  var pubElems = document.querySelectorAll(".publication");
  var yearElems = document.querySelectorAll(".year");

  var clearElem = document.getElementById("clear-filters");
  var highlightElem = document.getElementById("highlight");

  var data = [];
  var allYears = {};

  pubElems.forEach(function(element) {
    var item = JSON.parse(element.getAttribute("data-pub"));

    if (item.highlight) {
      item.highlight = "Yes";
    }

    allYears[item.year] = 1;

    item.element = element;

    data.push(item);
  });

  var engine = itemsjs(data, {
    aggregations: {
      venue_tags: {
        size: 5
      },
      authors: {
        size: 6
      },
      awards: {
        size: 5
      },
      highlight: {
        size: 1
      },
      tags: {
        size: 6
      },
      type: {
        size: 5
      }
    }
  });

  var query = { filters: {} };

  function setAggs(aggs) {
    document.querySelectorAll("#facets > .facet").forEach(function(facet) {
      var id = facet.getAttribute("id");

      var buckets = aggs[id].buckets;

      var el = facet.querySelector("ul");
      if (buckets.length === 0) {
        el.innerHTML = "Empty";
      } else {
        el.innerHTML = "";

        buckets.forEach(function(bucket) {
          if (query.filters[id] && query.filters[id].indexOf(bucket.key) >= 0) {
            bucket.in_query = true;
          }
        });

        var maxDocCount = Math.max.apply(
          null,
          buckets.map(function(bucket) {
            return bucket.doc_count;
          })
        );

        buckets.forEach(function(bucket) {
          var child = document.createElement("li");
          child.classList.add("mb2", "pointer");

          var wrap = document.createElement("span");
          child.appendChild(wrap);

          var text = document.createElement("span");
          text.innerText = bucket.key;
          text.setAttribute("title", bucket.key);
          var number = document.createElement("span");
          number.classList.add("gray", "f6");
          number.innerText = " (" + bucket.doc_count + ")";
          wrap.appendChild(text);
          wrap.appendChild(number);

          var barFull = document.createElement("div");
          barFull.classList.add("w-100", "bb", "b--black-20", "bw1", "mt1");
          child.append(barFull);

          var bar = document.createElement("div");
          bar.classList.add("bb", "b--cmu-red", "bw1");
          bar.style.marginBottom = "-.125rem";
          bar.style.width = "" + (bucket.doc_count / maxDocCount) * 100 + "%";
          barFull.append(bar);

          if (bucket.in_query) {
            child.classList.add("b");

            // remove filter
            child.onclick = function() {
              query.filters[id].splice(
                query.filters[id].indexOf(bucket.key),
                1
              );
              if (query.filters[id].length === 0) {
                delete query.filters[id];
              }
              search(query);
            };
          } else {
            // add to filter
            child.onclick = function() {
              if (query.filters[id]) {
                query.filters[id].push(bucket.key);
              } else {
                query.filters[id] = [bucket.key];
              }
              search(query);
            };
          }

          el.appendChild(child);
        });
      }
    });
  }

  // full text search is broken
  // var ftSearch = document.getElementById("ft-search");
  // ftSearch.oninput = function() {
  //   var val = ftSearch.value;

  //   if (val) {
  //     query.query = val;
  //   } else {
  //     delete query.val;
  //   }

  //   console.log(query);
  //   search(query);
  // }

  function search(query) {
    console.time("Search");

    var result = engine.search(Object.assign({ per_page: 1000 }, query));

    setAggs(result.data.aggregations);

    var counter = pubElems.length - result.data.items.length;

    document.getElementById("count_hidden").innerText = counter;
    document.getElementById("count_total").innerText = pubElems.length;

    pubElems.forEach(function(element) {
      element.classList.add("dn");
    });

    var visibleYears = {};
    result.data.items.forEach(function(item) {
      item.element.classList.remove("dn");
      visibleYears[item.year] = 1;
    });

    yearElems.forEach(function(element) {
      element.classList.add("dn");
    });
    Object.keys(allYears).forEach(function(year) {
      if (year in visibleYears) {
        document.getElementById("y" + year).classList.remove("dn");
      }
    });

    // show or hide notification about filtered papers
    if (Object.keys(query.filters).length || query.query) {
      clearElem.classList.remove("dn");
    } else {
      clearElem.classList.add("dn");
    }

    highlightElem.checked = !!query.filters.highlight;

    console.timeEnd("Search");
  }

  highlightElem.onchange = function() {
    if (highlightElem.checked) {
      query.filters.highlight = ["Yes"];
    } else {
      delete query.filters.highlight;
    }
    search(query);
  };

  clearElem.onclick = function() {
    query = { filters: {} };
    search(query);
  };

  search(query);

  document.getElementById("facets").classList.remove("dn");
  document.getElementById("only-highlight").classList.remove("dn");
})();

</script>

<script>
  //From https://codepen.io/Anotherdago/pen/yjVxOB
  //Add basic styles for active tabs
    $('.accordion__item-label').on('click', function() {
      $(this).addClass('blue');
      $(this).parent('.accordion__item').siblings().find('.accordion__item-label').removeClass('blue');
    });
</script>

  </div>
</section>

      </main>
    </div>

    <footer class="bt white mt5 flex-shrink-0">
      <div class="w-100 mw8 ph4-l ph3 center pv3 footerinfo">
        <ul class="list pl0">
          <li>
            <a href="/index" class="link white dib underline-dot-white hover-cmu-red mv1">BEAR Lab</a>, 
            <a href="https://www.cs.rochester.edu" class="link white dib underline-dot-white hover-cmu-red mv1">Department of Computer Science</a>,
            <a href="https://rochester.edu/" class="link white dib underline-dot-white hover-cmu-red mv1">University of Rochester</a>
            <!-- <span ></span>  -->
          </li>
          <li>
            <span>
            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, <a href="/contact" class="link white dim underline-dot-white hover-cmu-red">How to find us.</a>
            </span>
          </li>
          <li class="pv1">
            <abbr title="Last build on 2023-10-05" class="white" style="list-style: height 2em; text-decoration: none;">Last update October 2023</abbr>              
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
