<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="application-name" content="URCS BEAR Lab" />
  <meta name="theme-color" content="#b00" />
  
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap"
    rel="stylesheet"
  />
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&display=swap" rel="stylesheet">

  <link
    href="https://use.fontawesome.com/releases/v5.13.0/css/all.css"
    rel="stylesheet"
  />
  <link href="/styles.css" rel="stylesheet" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="shortcut icon" href="/logo.png" />
  <link
    rel="icon"
    type="image/png"
    href="/assets/logo.png"
    sizes="250x250"
  />

  <link
    rel="alternate"
    type="application/rss+xml"
    title="URCS BEAR Lab"
    href="http://localhost:4000/feed.xml"
  />

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices | URCS BEAR Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices" />
<meta name="author" content="Yukang Yan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices." />
<meta property="og:description" content="We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices." />
<link rel="canonical" href="http://localhost:4000/publications/2018-headgesture.html" />
<meta property="og:url" content="http://localhost:4000/publications/2018-headgesture.html" />
<meta property="og:site_name" content="URCS BEAR Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-10T17:05:26-04:00" />
<script type="application/ld+json">
{"headline":"HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices","dateModified":"2023-10-10T17:05:26-04:00","datePublished":"2023-10-10T17:05:26-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/publications/2018-headgesture.html"},"url":"http://localhost:4000/publications/2018-headgesture.html","author":{"@type":"Person","name":"Yukang Yan"},"description":"We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users’ hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-MSRQ58DGER"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MSRQ58DGER');
  </script>
</head>



  <body>
    <div class="content">
      <header class="bg-white">
        <div class="w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l">
        <!-- <div class="headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l"> -->
        <!-- <div class="headermenu w-100 mw8 pa2 flex flex-column">           -->
          <a href="/" class="dib f2 mt4 fw6 link cmu-dark-gray hover-cmu-red">
          <!-- <a href="/" class="dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2"> -->
            <!-- <picture> -->
              <!-- <source  
                srcset="/assets/logo-sphere-01-01-01.svg"
                type="image/svg+xml"              
              /> -->
              <!-- <source
                srcset="/assets/logo-light.webp"
                type="image/webp"
              /> -->
              <!-- <source  
                srcset="/assets/logo-sphere-03-01.png"
                type="image/png"
              />
              <img 
                class="logo"
                alt="A coarsely tesselated sphere colored in shades of gray."
                class="pl2 w3"
              /> -->              
            <!-- </picture> -->
              BEAR Lab
          </a>
          <!--  --><nav class="mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns">
            <!-- <nav class="mt2 lh-copy w-100 w-25 pa3 mr2"></nav> -->
            <div class="tc mt0-ns mt2">
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/index">Home</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/team">Team</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/publications">Publications</a>
              <!-- <a
                class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  "
                href="/teaching"
                >Teaching</a
              > -->
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b ur-blue hover-cmu-red link dib  " href="/contact">Contact</a>
            </div>
          </nav>
        </div>

      </header>

      <main>
        <section>
  <div class="pv4 bg-white">
    <div class="w-100 mw8 ph4-l ph3 pv2-l pv3 center">
      <h1 class="f2 lh-title measure mt3">
        HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices
      </h1>

      

      <div class="mb2">
        
          Yukang Yan,
          Chun Yu,
          Xin Yi,
          Yuanchun Shi.
        <!-- . -->
        </div>

      <!-- <div class="flex flex-row flex-wrap items-start mt1">
       
        <div class="flex flex-column items-center mt3 mr4">
          <picture>
            <source srcset=""></source>
            <source srcset="/assets/person.png"></source>
            <img class="br-100 w3 h3 mb1" alt="Picture of Yukang Yan" />
          </picture>
          <div class="black mw4 tc">Yukang Yan</div>
        </div>
      
      </div> -->

      
        <div class="mt3">
          Published at
          <span class="b">
            
              <a href="https://ubicomp.org/ubicomp2019/" class="black underline-dot hover-cmu-red link">
            
            ACM IMWUT
            2018
            </a>
          </span>
        </div>
      

      
    </div>
  </div>

  <div class="w-100 mw8 ph4-l ph3 center mt1">
    
    <img src="/assets/publications/2018-headgesture.png" alt="Teaser image" class="mw-100" style="max-height: 600px">

    

    
    
      <h2>Abstract</h2>
      <div class="lh-copy">
        We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.
      </div>
    

    <!-- width="400" height="240" class="thumb-video"  -->
    

    
    <h2>Materials</h2>
      <ul class="list pl0">
        
          <li class="mt2"><a href="https://dl.acm.org/doi/pdf/10.1145/3287076" class="black link hover-cmu-red underline-dot ">
            PDF
          </a></li>
        

        

        

        

        

        

        

        

        
        
      </ul>
    

    
      <h2>Bibtex</h2>
      <div class="f6 underline-dot">
        <pre>@article{yukang20181, author = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun}, title = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices}, year = {2018}, issue_date = {December 2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {2}, number = {4}, url = {https://doi.org/10.1145/3287076}, doi = {10.1145/3287076}, abstract = {}, journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}, month = {dec}, articleno = {198}, numpages = {23}, keywords = {Gesture, Virtual Reality, Head Movement Interaction} }</pre>
      </div>
    
    
  </div>
</section>

      </main>
    </div>

    <footer class="bt white mt5 flex-shrink-0">
      <div class="w-100 mw8 ph4-l ph3 center pv3 footerinfo">
        <ul class="list pl0">
          <li>
            <a href="/index" class="link white dib underline-dot-white hover-cmu-red mv1">BEAR Lab</a>, 
            <a href="https://www.cs.rochester.edu" class="link white dib underline-dot-white hover-cmu-red mv1">Department of Computer Science</a>,
            <a href="https://rochester.edu/" class="link white dib underline-dot-white hover-cmu-red mv1">University of Rochester</a>
            <!-- <span ></span>  -->
          </li>
          <li>
            <span>
            Wegmans Hall, 250 Hutchison Rd, Rochester, NY 14620 United States, <a href="/contact" class="link white dim underline-dot-white hover-cmu-red">How to find us.</a>
            </span>
          </li>
          <li class="pv1">
            <abbr title="Last build on 2023-10-10" class="white" style="list-style: height 2em; text-decoration: none;">Last update October 2023</abbr>              
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
