---
layout: publication
year: 2022
month: 10
selected: true
coming-soon: false
hidden: false
external : false
# link: https://dl.acm.org/doi/abs/10.1145/3526113.3545646
pdf: https://dl.acm.org/doi/abs/10.1145/3534590
title: "Color-to-Depth Mappings as Depth Cues in Virtual Reality"
authors:
  - Zhipeng Li
  - Yikai Cui
  - Tianze Zhou
  - Yu Jiang
  - Yuntao Wang
  - Yukang Yan
  - Michael Nebeling
  - Yuanchun Shi
# blog: https://ait.ethz.ch/projects/2020/omni/
doi: 10.1145/3526113.3545646
venue_location: Virtual
venue_url: https://uist.acm.org/uist2022/
venue_tags:
  - ACM UIST
type:
  - Conference
tags:
  - Science
  - Mixed Reality
  - Perception
venue: ACM UIST

#video-thumb: 4E5Ca-bVsxU
#video-30sec: 4E5Ca-bVsxU
#video-suppl: vRyM7TEQHWM
#video-talk-5min: UvP8FJ9Q0x8
#video-talk-15min: V7Qi6Mdsxak

bibtex: "@inproceedings{zhipeng20222,
author = {Li, Zhipeng and Cui, Yikai and Zhou, Tianze and Jiang, Yu and Wang, Yuntao and Yan, Yukang and Nebeling, Michael and Shi, Yuanchun},
title = {Color-to-Depth Mappings as Depth Cues in Virtual Reality},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545646},
doi = {10.1145/3526113.3545646},
abstract = {},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {80},
numpages = {14},
keywords = {Virtual reality, color-to-depth mapping, depth perception},
location = {Bend, OR, USA},
series = {UIST '22}
}"

---

Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans’ inherent depth cues (e.g., retinal blur, motion parallax), we investigate users’ perceptual mappings of distance to virtual objects’ appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users’ strategies of mapping a virtual object’s hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.
